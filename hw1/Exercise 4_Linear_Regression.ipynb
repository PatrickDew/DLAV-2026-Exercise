{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intro to Python: Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear regression with one variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by importing some libraries and examining the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/patrickdew/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the data from the CSV file using Panda library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Population</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.1101</td>\n",
              "      <td>17.5920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.5277</td>\n",
              "      <td>9.1302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.5186</td>\n",
              "      <td>13.6620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.0032</td>\n",
              "      <td>11.8540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.8598</td>\n",
              "      <td>6.8233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Population   Profit\n",
              "0      6.1101  17.5920\n",
              "1      5.5277   9.1302\n",
              "2      8.5186  13.6620\n",
              "3      7.0032  11.8540\n",
              "4      5.8598   6.8233"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data', 'ex1data1.txt')\n",
        "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Population</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>97.000000</td>\n",
              "      <td>97.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.159800</td>\n",
              "      <td>5.839135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.869884</td>\n",
              "      <td>5.510262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.026900</td>\n",
              "      <td>-2.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.707700</td>\n",
              "      <td>1.986900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.589400</td>\n",
              "      <td>4.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.578100</td>\n",
              "      <td>7.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>22.203000</td>\n",
              "      <td>24.147000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Population     Profit\n",
              "count   97.000000  97.000000\n",
              "mean     8.159800   5.839135\n",
              "std      3.869884   5.510262\n",
              "min      5.026900  -2.680700\n",
              "25%      5.707700   1.986900\n",
              "50%      6.589400   4.562300\n",
              "75%      8.578100   7.046700\n",
              "max     22.203000  24.147000"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot it to get a better idea of what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='Population', ylabel='Profit'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
        "\n",
        "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
        "\n",
        "where $\\theta$ and $x_n$ are vectors\n",
        "\n",
        "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeCost(x, y, theta):\n",
        "    # Compute the cost function\n",
        "    N = len(y)\n",
        "    predictions = x @ theta\n",
        "    cost = (1 / N) * np.sum((y - predictions) ** 2)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.insert(0, 'Ones', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's do some variable initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set X (training data) and y (target variable)\n",
        "cols = data.shape[1]\n",
        "X = data.iloc[:,0:cols-1]\n",
        "y = data.iloc[:,cols-1:cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look to make sure X (training set) and y (target variable) look correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ones</th>\n",
              "      <th>Population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>6.1101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5.5277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>8.5186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5.8598</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ones  Population\n",
              "0     1      6.1101\n",
              "1     1      5.5277\n",
              "2     1      8.5186\n",
              "3     1      7.0032\n",
              "4     1      5.8598"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.5920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.1302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.6620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.8540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.8233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Profit\n",
              "0  17.5920\n",
              "1   9.1302\n",
              "2  13.6620\n",
              "3  11.8540\n",
              "4   6.8233"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.array(X.values)\n",
        "y = np.array(y.values).flatten()\n",
        "theta = np.array([0,0])\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a quick look at the shape of our matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((97, 2), (2,), (97,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape, theta.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's compute the cost for our initial solution (0 values for theta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64.14546775491135"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "computeCost(X, y, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
        "\n",
        "The gradient descent formula is:\n",
        "\n",
        "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
        "\n",
        "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
        "\n",
        "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient(y, tx, w):\n",
        "    \"\"\"Compute the gradient.\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute gradient and loss\n",
        "    # ***************************************************\n",
        "    N = len(y)\n",
        "    error = y - tx @ w\n",
        "    gradient = (-2 / N) * (tx.T @ error)\n",
        "    return gradient\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradientDescent(X, y, theta, alpha, max_iters):\n",
        "    \"\"\"Gradient descent algorithm.\"\"\"\n",
        "    # Define parameters to store w and loss\n",
        "    ws = [theta]\n",
        "    cost = np.zeros(max_iters)\n",
        "    for n_iter in range(max_iters):\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: compute gradient and loss\n",
        "        # ***************************************************\n",
        "        gradient = compute_gradient(y, X, theta)\n",
        "        loss = computeCost(X, y, theta)\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: update theta by gradient\n",
        "        # ***************************************************\n",
        "        theta = theta - alpha * gradient\n",
        "        # store w and loss\n",
        "        ws.append(theta)\n",
        "        cost[n_iter] = loss\n",
        "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
        "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
        "        \n",
        "\n",
        "    return theta, cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = 0.01\n",
        "iters = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Descent(0/999): loss=64.14546775491135, w0=0.11678270103092776, w1=1.3065769949111339\n",
            "Gradient Descent(1/999): loss=33.53928474333491, w0=0.018001608779719652, w1=0.46688514041747775\n",
            "Gradient Descent(2/999): loss=20.815159503537934, w0=0.05823049025948241, w1=1.010398519281903\n",
            "Gradient Descent(3/999): loss=15.518366960604673, w0=0.008955584732491154, w1=0.6624640652822505\n",
            "Gradient Descent(4/999): loss=13.3065750692935, w0=0.017447688470967004, w1=0.8890358067492414\n",
            "Gradient Descent(5/999): loss=12.376211646075982, w0=-0.011205651785773704, w1=0.7453450091050499\n",
            "Gradient Descent(6/999): loss=11.978169038728407, w0=-0.015836161825038116, w1=0.840270271487082\n",
            "Gradient Descent(7/999): loss=11.801307545851573, w0=-0.03586548478321536, w1=0.7814054019591586\n",
            "Gradient Descent(8/999): loss=11.716395840326452, w0=-0.04588771003475006, w1=0.8216458872863022\n",
            "Gradient Descent(9/999): loss=11.669757570402808, w0=-0.0622765770247026, w1=0.7980072763515388\n",
            "Gradient Descent(10/999): loss=11.639097868446488, w0=-0.07447993992474644, w1=0.8155287856125967\n",
            "Gradient Descent(11/999): loss=11.615156054554555, w0=-0.089298675792157, w1=0.8065154361246224\n",
            "Gradient Descent(12/999): loss=11.594085154771326, w0=-0.1023500943591799, w1=0.8145948887823524\n",
            "Gradient Descent(13/999): loss=11.57428676210951, w0=-0.11645901891079326, w1=0.811650294700878\n",
            "Gradient Descent(14/999): loss=11.555096407741408, w0=-0.12980521899565403, w1=0.8158022520353793\n",
            "Gradient Descent(15/999): loss=11.536237534751157, w0=-0.14356207790797887, w1=0.8153725418897819\n",
            "Gradient Descent(16/999): loss=11.517594708120669, w0=-0.15697367266513632, w1=0.8178874980936855\n",
            "Gradient Descent(17/999): loss=11.499119418305693, w0=-0.17052746631980287, w1=0.8184966258951271\n",
            "Gradient Descent(18/999): loss=11.480790957069816, w0=-0.18390959132206014, w1=0.8203259705114382\n",
            "Gradient Descent(19/999): loss=11.46260016737746, w0=-0.19732261554827576, w1=0.8213609071086568\n",
            "Gradient Descent(20/999): loss=11.444542693942985, w0=-0.21063627680288677, w1=0.8228998372158082\n",
            "Gradient Descent(21/999): loss=11.42661617984241, w0=-0.22393481207017224, w1=0.8241060040160729\n",
            "Gradient Descent(22/999): loss=11.408819102107842, w0=-0.237164218229248, w1=0.8255187039129652\n",
            "Gradient Descent(23/999): loss=11.391150288067552, w0=-0.25035958323751545, w1=0.8267904031447669\n",
            "Gradient Descent(24/999): loss=11.373608714431438, w0=-0.2634985771734507, w1=0.8281451065334089\n",
            "Gradient Descent(25/999): loss=11.356193423813906, w0=-0.27659587340488007, w1=0.8294384772326225\n",
            "Gradient Descent(26/999): loss=11.33890349003421, w0=-0.2896422966363097, w1=0.8307635647986902\n",
            "Gradient Descent(27/999): loss=11.321738003676492, w0=-0.3026440403935427, w1=0.8320604294011863\n",
            "Gradient Descent(28/999): loss=11.304696066064789, w0=-0.3155973923913, w1=0.8333677287388969\n",
            "Gradient Descent(29/999): loss=11.287776786733453, w0=-0.3285050233718192, w1=0.8346605768984672\n",
            "Gradient Descent(30/999): loss=11.27097928234944, w0=-0.3413654893809772, w1=0.8359550400503488\n",
            "Gradient Descent(31/999): loss=11.254302676237955, w0=-0.3541799972784866, w1=0.8372407917250402\n",
            "Gradient Descent(32/999): loss=11.237746098158803, w0=-0.3669480445483487, w1=0.8385245147143746\n",
            "Gradient Descent(33/999): loss=11.221308684187079, w0=-0.37967022932978095, w1=0.8398019296894985\n",
            "Gradient Descent(34/999): loss=11.204989576637416, w0=-0.3923464394298649, w1=0.8410758220203125\n",
            "Gradient Descent(35/999): loss=11.1887879240065, w0=-0.4049770194607667, w1=0.8423444239654388\n",
            "Gradient Descent(36/999): loss=11.172702880923309, w0=-0.41756201865408726, w1=0.8436089020451879\n",
            "Gradient Descent(37/999): loss=11.156733608102838, w0=-0.4301016756282442, w1=0.8448685316272172\n",
            "Gradient Descent(38/999): loss=11.140879272301378, w0=-0.4425961059721868, w1=0.8461238067793512\n",
            "Gradient Descent(39/999): loss=11.125139046272663, w0=-0.45504550359297824, w1=0.8473744360516899\n",
            "Gradient Descent(40/999): loss=11.109512108724548, w0=-0.4674500109560824, w1=0.8486206341426101\n",
            "Gradient Descent(41/999): loss=11.093997644276083, w0=-0.47980980271557033, w1=0.8498622894543948\n",
            "Gradient Descent(42/999): loss=11.078594843414963, w0=-0.4921250318201305, w1=0.8510995005820563\n",
            "Gradient Descent(43/999): loss=11.063302902455252, w0=-0.5043958642497893, w1=0.8523322305607661\n",
            "Gradient Descent(44/999): loss=11.048121023495467, w0=-0.5166224566324605, w1=0.8535605296946278\n",
            "Gradient Descent(45/999): loss=11.033048414376907, w0=-0.528804970672928, w1=0.8547843919497466\n",
            "Gradient Descent(46/999): loss=11.018084288642372, w0=-0.5409435638571725, w1=0.8560038475054581\n",
            "Gradient Descent(47/999): loss=11.003227865495084, w0=-0.5530383954466019, w1=0.8572189031078419\n",
            "Gradient Descent(48/999): loss=10.988478369757956, w0=-0.5650896226183294, w1=0.8584295805106527\n",
            "Gradient Descent(49/999): loss=10.973835031833165, w0=-0.5770974029560515, w1=0.8596358917014933\n",
            "Gradient Descent(50/999): loss=10.959297087661964, w0=-0.5890618928481195, w1=0.8608378548682327\n",
            "Gradient Descent(51/999): loss=10.944863778684844, w0=-0.6009832485233054, w1=0.8620354841094084\n",
            "Gradient Descent(52/999): loss=10.930534351801933, w0=-0.6128616253866305, w1=0.8632287960659379\n",
            "Gradient Descent(53/999): loss=10.91630805933372, w0=-0.6246971784507468, w1=0.8644178056474904\n",
            "Gradient Descent(54/999): loss=10.902184158982017, w0=-0.6364900620612519, w1=0.8656025287870628\n",
            "Gradient Descent(55/999): loss=10.888161913791253, w0=-0.6482404300770326, w1=0.8667829806660607\n",
            "Gradient Descent(56/999): loss=10.874240592110011, w0=-0.6599484357573425, w1=0.8679591768585191\n",
            "Gradient Descent(57/999): loss=10.860419467552845, w0=-0.6716142318378707, w1=0.8691311325940132\n",
            "Gradient Descent(58/999): loss=10.84669781896239, w0=-0.683237970484998, w1=0.8702988632330222\n",
            "Gradient Descent(59/999): loss=10.83307493037173, w0=-0.6948198033285465, w1=0.8714623839608985\n",
            "Gradient Descent(60/999): loss=10.81955009096704, w0=-0.7063598814439306, w1=0.8726217099854241\n",
            "Gradient Descent(61/999): loss=10.806122595050484, w0=-0.7178583553669055, w1=0.8737768564098156\n",
            "Gradient Descent(62/999): loss=10.79279174200342, w0=-0.7293153750872958, w1=0.8749278383148955\n",
            "Gradient Descent(63/999): loss=10.77955683624983, w0=-0.7407310900562598, w1=0.8760746707064573\n",
            "Gradient Descent(64/999): loss=10.76641718722001, w0=-0.7521056491848178, w1=0.8772173685495103\n",
            "Gradient Descent(65/999): loss=10.753372109314563, w0=-0.7634392008479995, w1=0.878355946746532\n",
            "Gradient Descent(66/999): loss=10.740420921868619, w0=-0.7747318928853587, w1=0.8794904201518022\n",
            "Gradient Descent(67/999): loss=10.727562949116308, w0=-0.7859838726038172, w1=0.880620803562486\n",
            "Gradient Descent(68/999): loss=10.71479752015551, w0=-0.7971952867789964, w1=0.8817471117246971\n",
            "Gradient Descent(69/999): loss=10.702123968912844, w0=-0.8083662816575123, w1=0.8828693593299077\n",
            "Gradient Descent(70/999): loss=10.689541634108926, w0=-0.8194970029586379, w1=0.8839875610175774\n",
            "Gradient Descent(71/999): loss=10.67704985922384, w0=-0.8305875958763619, w1=0.8851017313737736\n",
            "Gradient Descent(72/999): loss=10.664647992462907, w0=-0.8416382050811811, w1=0.886211884932374\n",
            "Gradient Descent(73/999): loss=10.652335386722667, w0=-0.8526489747220534, w1=0.8873180361746038\n",
            "Gradient Descent(74/999): loss=10.640111399557108, w0=-0.8636200484282351, w1=0.8884201995296448\n",
            "Gradient Descent(75/999): loss=10.627975393144157, w0=-0.8745515693111825, w1=0.8895183893745532\n",
            "Gradient Descent(76/999): loss=10.61592673425238, w0=-0.8854436799664006, w1=0.8906126200346213\n",
            "Gradient Descent(77/999): loss=10.603964794207966, w0=-0.8962965224753148, w1=0.8917029057834518\n",
            "Gradient Descent(78/999): loss=10.592088948861903, w0=-0.9071102384071168, w1=0.8927892608432169\n",
            "Gradient Descent(79/999): loss=10.580298578557429, w0=-0.9178849688206162, w1=0.8938716993847968\n",
            "Gradient Descent(80/999): loss=10.56859306809769, w0=-0.9286208542660773, w1=0.8949502355279948\n",
            "Gradient Descent(81/999): loss=10.556971806713634, w0=-0.9393180347870546, w1=0.8960248833417019\n",
            "Gradient Descent(82/999): loss=10.545434188032146, w0=-0.949976649922218, w1=0.8970956568440931\n",
            "Gradient Descent(83/999): loss=10.533979610044433, w0=-0.9605968387071744, w1=0.8981625700028019\n",
            "Gradient Descent(84/999): loss=10.52260747507457, w0=-0.9711787396762803, w1=0.8992256367351077\n",
            "Gradient Descent(85/999): loss=10.511317189748349, w0=-0.9817224908644495, w1=0.9002848709081145\n",
            "Gradient Descent(86/999): loss=10.500108164962299, w0=-0.9922282298089533, w1=0.9013402863389334\n",
            "Gradient Descent(87/999): loss=10.488979815852952, w0=-1.002696093551215, w1=0.9023918967948614\n",
            "Gradient Descent(88/999): loss=10.477931561766338, w0=-1.0131262186385972, w1=0.9034397159935627\n",
            "Gradient Descent(89/999): loss=10.466962826227665, w0=-1.0235187411261828, w1=0.9044837576032456\n",
            "Gradient Descent(90/999): loss=10.456073036911265, w0=-1.0338737965785505, w1=0.9055240352428429\n",
            "Gradient Descent(91/999): loss=10.445261625610708, w0=-1.0441915200715426, w1=0.9065605624821879\n",
            "Gradient Descent(92/999): loss=10.434528028209174, w0=-1.0544720461940271, w1=0.9075933528421922\n",
            "Gradient Descent(93/999): loss=10.423871684650003, w0=-1.0647155090496532, w1=0.9086224197950221\n",
            "Gradient Descent(94/999): loss=10.413292038907475, w0=-1.0749220422586008, w1=0.909647776764274\n",
            "Gradient Descent(95/999): loss=10.402788538957822, w0=-1.0850917789593233, w1=0.9106694371251497\n",
            "Gradient Descent(96/999): loss=10.392360636750373, w0=-1.095224851810285, w1=0.9116874142046307\n",
            "Gradient Descent(97/999): loss=10.382007788179019, w0=-1.1053213929916903, w1=0.9127017212816524\n",
            "Gradient Descent(98/999): loss=10.37172945305377, w0=-1.115381534207209, w1=0.913712371587277\n",
            "Gradient Descent(99/999): loss=10.36152509507261, w0=-1.1254054066856942, w1=0.914719378304866\n",
            "Gradient Descent(100/999): loss=10.351394181793477, w0=-1.1353931411828935, w1=0.915722754570253\n",
            "Gradient Descent(101/999): loss=10.341336184606522, w0=-1.1453448679831548, w1=0.9167225134719134\n",
            "Gradient Descent(102/999): loss=10.331350578706479, w0=-1.1552607169011262, w1=0.9177186680511371\n",
            "Gradient Descent(103/999): loss=10.321436843065332, w0=-1.1651408172834492, w1=0.9187112313021967\n",
            "Gradient Descent(104/999): loss=10.31159446040509, w0=-1.1749852980104456, w1=0.9197002161725192\n",
            "Gradient Descent(105/999): loss=10.301822917170815, w0=-1.1847942874977992, w1=0.9206856355628522\n",
            "Gradient Descent(106/999): loss=10.292121703503827, w0=-1.1945679136982306, w1=0.921667502327435\n",
            "Gradient Descent(107/999): loss=10.282490313215087, w0=-1.2043063041031663, w1=0.9226458292741638\n",
            "Gradient Descent(108/999): loss=10.272928243758807, w0=-1.2140095857444015, w1=0.9236206291647606\n",
            "Gradient Descent(109/999): loss=10.263434996206195, w0=-1.223677885195758, w1=0.9245919147149386\n",
            "Gradient Descent(110/999): loss=10.254010075219462, w0=-1.2333113285747341, w1=0.9255596985945684\n",
            "Gradient Descent(111/999): loss=10.244652989025942, w0=-1.2429100415441507, w1=0.9265239934278436\n",
            "Gradient Descent(112/999): loss=10.235363249392456, w0=-1.2524741493137903, w1=0.9274848117934449\n",
            "Gradient Descent(113/999): loss=10.226140371599822, w0=-1.2620037766420298, w1=0.9284421662247044\n",
            "Gradient Descent(114/999): loss=10.21698387441757, w0=-1.2714990478374681, w1=0.9293960692097694\n",
            "Gradient Descent(115/999): loss=10.207893280078833, w0=-1.2809600867605484, w1=0.9303465331917646\n",
            "Gradient Descent(116/999): loss=10.19886811425542, w0=-1.2903870168251728, w1=0.9312935705689555\n",
            "Gradient Descent(117/999): loss=10.189907906033053, w0=-1.2997799610003127, w1=0.9322371936949088\n",
            "Gradient Descent(118/999): loss=10.181012187886813, w0=-1.309139041811613, w1=0.9331774148786547\n",
            "Gradient Descent(119/999): loss=10.17218049565674, w0=-1.3184643813429897, w1=0.9341142463848466\n",
            "Gradient Descent(120/999): loss=10.163412368523602, w0=-1.3277561012382235, w1=0.9350477004339222\n",
            "Gradient Descent(121/999): loss=10.154707348984875, w0=-1.3370143227025455, w1=0.9359777892022608\n",
            "Gradient Descent(122/999): loss=10.146064982830847, w0=-1.3462391665042188, w1=0.9369045248223453\n",
            "Gradient Descent(123/999): loss=10.137484819120935, w0=-1.355430752976114, w1=0.9378279193829172\n",
            "Gradient Descent(124/999): loss=10.12896641016014, w0=-1.3645892020172785, w1=0.9387479849291366\n",
            "Gradient Descent(125/999): loss=10.120509311475708, w0=-1.3737146330945005, w1=0.939664733462738\n",
            "Gradient Descent(126/999): loss=10.112113081793918, w0=-1.3828071652438676, w1=0.9405781769421877\n",
            "Gradient Descent(127/999): loss=10.103777283017072, w0=-1.3918669170723197, w1=0.9414883272828394\n",
            "Gradient Descent(128/999): loss=10.095501480200632, w0=-1.4008940067591957, w1=0.9423951963570893\n",
            "Gradient Descent(129/999): loss=10.08728524153051, w0=-1.4098885520577755, w1=0.9432987959945319\n",
            "Gradient Descent(130/999): loss=10.07912813830057, w0=-1.4188506702968158, w1=0.9441991379821133\n",
            "Gradient Descent(131/999): loss=10.071029744890225, w0=-1.4277804783820807, w1=0.9450962340642856\n",
            "Gradient Descent(132/999): loss=10.062989638742248, w0=-1.4366780927978664, w1=0.9459900959431599\n",
            "Gradient Descent(133/999): loss=10.055007400340724, w0=-1.4455436296085211, w1=0.9468807352786586\n",
            "Gradient Descent(134/999): loss=10.04708261318915, w0=-1.4543772044599588, w1=0.9477681636886686\n",
            "Gradient Descent(135/999): loss=10.039214863788711, w0=-1.4631789325811677, w1=0.9486523927491912\n",
            "Gradient Descent(136/999): loss=10.031403741616698, w0=-1.4719489287857135, w1=0.949533433994495\n",
            "Gradient Descent(137/999): loss=10.023648839105103, w0=-1.480687307473237, w1=0.9504112989172641\n",
            "Gradient Descent(138/999): loss=10.015949751619333, w0=-1.4893941826309463, w1=0.9512859989687503\n",
            "Gradient Descent(139/999): loss=10.008306077437112, w0=-1.4980696678351038, w1=0.9521575455589213\n",
            "Gradient Descent(140/999): loss=10.000717417727506, w0=-1.5067138762525076, w1=0.9530259500566092\n",
            "Gradient Descent(141/999): loss=9.993183376530133, w0=-1.515326920641968, w1=0.9538912237896596\n",
            "Gradient Descent(142/999): loss=9.98570356073448, w0=-1.523908913355778, w1=0.954753378045079\n",
            "Gradient Descent(143/999): loss=9.978277580059409, w0=-1.5324599663411793, w1=0.9556124240691822\n",
            "Gradient Descent(144/999): loss=9.970905047032781, w0=-1.5409801911418222, w1=0.9564683730677386\n",
            "Gradient Descent(145/999): loss=9.96358557697125, w0=-1.5494696988992205, w1=0.9573212362061191\n",
            "Gradient Descent(146/999): loss=9.956318787960175, w0=-1.557928600354202, w1=0.9581710246094405\n",
            "Gradient Descent(147/999): loss=9.94910430083371, w0=-1.5663570058483525, w1=0.9590177493627126\n",
            "Gradient Descent(148/999): loss=9.941941739155011, w0=-1.5747550253254547, w1=0.9598614215109803\n",
            "Gradient Descent(149/999): loss=9.934830729196591, w0=-1.5831227683329239, w1=0.9607020520594702\n",
            "Gradient Descent(150/999): loss=9.927770899920827, w0=-1.5914603440232349, w1=0.9615396519737319\n",
            "Gradient Descent(151/999): loss=9.920761882960596, w0=-1.5997678611553474, w1=0.9623742321797832\n",
            "Gradient Descent(152/999): loss=9.913803312600058, w0=-1.6080454280961245, w1=0.9632058035642502\n",
            "Gradient Descent(153/999): loss=9.906894825755565, w0=-1.6162931528217455, w1=0.964034376974512\n",
            "Gradient Descent(154/999): loss=9.900036061956726, w0=-1.6245111429191152, w1=0.9648599632188396\n",
            "Gradient Descent(155/999): loss=9.893226663327601, w0=-1.6326995055872668, w1=0.965682573066539\n",
            "Gradient Descent(156/999): loss=9.886466274568006, w0=-1.6408583476387606, w1=0.9665022172480904\n",
            "Gradient Descent(157/999): loss=9.879754542935007, w0=-1.648987775501077, w1=0.9673189064552885\n",
            "Gradient Descent(158/999): loss=9.87309111822448, w0=-1.6570878952180048, w1=0.9681326513413822\n",
            "Gradient Descent(159/999): loss=9.866475652752865, w0=-1.665158812451025, w1=0.9689434625212133\n",
            "Gradient Descent(160/999): loss=9.859907801339018, w0=-1.6732006324806887, w1=0.9697513505713552\n",
            "Gradient Descent(161/999): loss=9.853387221286187, w0=-1.68121346020799, w1=0.9705563260302505\n",
            "Gradient Descent(162/999): loss=9.846913572364146, w0=-1.689197400155735, w1=0.9713583993983489\n",
            "Gradient Descent(163/999): loss=9.84048651679144, w0=-1.6971525564699055, w1=0.9721575811382439\n",
            "Gradient Descent(164/999): loss=9.834105719217755, w0=-1.7050790329210164, w1=0.9729538816748093\n",
            "Gradient Descent(165/999): loss=9.827770846706418, w0=-1.7129769329054705, w1=0.9737473113953352\n",
            "Gradient Descent(166/999): loss=9.821481568717036, w0=-1.7208463594469063, w1=0.9745378806496638\n",
            "Gradient Descent(167/999): loss=9.81523755708824, w0=-1.7286874151975429, w1=0.9753255997503236\n",
            "Gradient Descent(168/999): loss=9.809038486020565, w0=-1.736500202439518, w1=0.9761104789726652\n",
            "Gradient Descent(169/999): loss=9.802884032059454, w0=-1.744284823086223, w1=0.9768925285549939\n",
            "Gradient Descent(170/999): loss=9.796773874078372, w0=-1.7520413786836313, w1=0.977671758698704\n",
            "Gradient Descent(171/999): loss=9.790707693262059, w0=-1.7597699704116245, w1=0.9784481795684128\n",
            "Gradient Descent(172/999): loss=9.784685173089885, w0=-1.7674706990853108, w1=0.9792218012920906\n",
            "Gradient Descent(173/999): loss=9.778705999319355, w0=-1.7751436651563408, w1=0.9799926339611958\n",
            "Gradient Descent(174/999): loss=9.772769859969687, w0=-1.7827889687142175, w1=0.9807606876308045\n",
            "Gradient Descent(175/999): loss=9.766876445305542, w0=-1.7904067094876022, w1=0.9815259723197424\n",
            "Gradient Descent(176/999): loss=9.761025447820879, w0=-1.797996986845615, w1=0.9822884980107156\n",
            "Gradient Descent(177/999): loss=9.755216562222888, w0=-1.8055598997991316, w1=0.9830482746504403\n",
            "Gradient Descent(178/999): loss=9.749449485416067, w0=-1.8130955470020744, w1=0.9838053121497734\n",
            "Gradient Descent(179/999): loss=9.743723916486417, w0=-1.8206040267526995, w1=0.9845596203838406\n",
            "Gradient Descent(180/999): loss=9.738039556685727, w0=-1.828085436994879, w1=0.9853112091921664\n",
            "Gradient Descent(181/999): loss=9.732396109415994, w0=-1.8355398753193783, w1=0.9860600883788019\n",
            "Gradient Descent(182/999): loss=9.726793280213945, w0=-1.8429674389651298, w1=0.9868062677124522\n",
            "Gradient Descent(183/999): loss=9.721230776735675, w0=-1.8503682248205007, w1=0.9875497569266051\n",
            "Gradient Descent(184/999): loss=9.71570830874139, w0=-1.857742329424557, w1=0.9882905657196567\n",
            "Gradient Descent(185/999): loss=9.71022558808026, w0=-1.8650898489683232, w1=0.9890287037550388\n",
            "Gradient Descent(186/999): loss=9.70478232867539, w0=-1.8724108792960361, w1=0.9897641806613446\n",
            "Gradient Descent(187/999): loss=9.699378246508894, w0=-1.8797055159063965, w1=0.9904970060324546\n",
            "Gradient Descent(188/999): loss=9.694013059607073, w0=-1.8869738539538132, w1=0.991227189427661\n",
            "Gradient Descent(189/999): loss=9.68868648802569, w0=-1.8942159882496457, w1=0.9919547403717937\n",
            "Gradient Descent(190/999): loss=9.683398253835389, w0=-1.9014320132634401, w1=0.9926796683553427\n",
            "Gradient Descent(191/999): loss=9.678148081107171, w0=-1.908622023124162, w1=0.9934019828345843\n",
            "Gradient Descent(192/999): loss=9.672935695898005, w0=-1.9157861116214236, w1=0.9941216932317017\n",
            "Gradient Descent(193/999): loss=9.667760826236538, w0=-1.9229243722067082, w1=0.9948388089349103\n",
            "Gradient Descent(194/999): loss=9.662623202108897, w0=-1.9300368979945879, w1=0.9955533392985791\n",
            "Gradient Descent(195/999): loss=9.657522555444597, w0=-1.9371237817639393, w1=0.9962652936433523\n",
            "Gradient Descent(196/999): loss=9.652458620102571, w0=-1.9441851159591532, w1=0.996974681256272\n",
            "Gradient Descent(197/999): loss=9.647431131857266, w0=-1.9512209926913409, w1=0.9976815113908979\n",
            "Gradient Descent(198/999): loss=9.642439828384862, w0=-1.9582315037395353, w1=0.9983857932674298\n",
            "Gradient Descent(199/999): loss=9.637484449249586, w0=-1.965216740551888, w1=0.9990875360728265\n",
            "Gradient Descent(200/999): loss=9.632564735890117, w0=-1.9721767942468635, w1=0.9997867489609258\n",
            "Gradient Descent(201/999): loss=9.62768043160611, w0=-1.9791117556144258, w1=1.0004834410525647\n",
            "Gradient Descent(202/999): loss=9.622831281544782, w0=-1.9860217151172237, w1=1.001177621435697\n",
            "Gradient Descent(203/999): loss=9.618017032687629, w0=-1.9929067628917714, w1=1.001869299165514\n",
            "Gradient Descent(204/999): loss=9.613237433837213, w0=-1.9997669887496234, w1=1.0025584832645598\n",
            "Gradient Descent(205/999): loss=9.608492235604071, w0=-2.0066024821785464, w1=1.0032451827228517\n",
            "Gradient Descent(206/999): loss=9.603781190393683, w0=-2.0134133323436862, w1=1.0039294064979956\n",
            "Gradient Descent(207/999): loss=9.59910405239356, w0=-2.0201996280887315, w1=1.0046111635153032\n",
            "Gradient Descent(208/999): loss=9.594460577560435, w0=-2.0269614579370727, w1=1.0052904626679104\n",
            "Gradient Descent(209/999): loss=9.589850523607497, w0=-2.033698910092956, w1=1.0059673128168891\n",
            "Gradient Descent(210/999): loss=9.585273649991786, w0=-2.040412072442634, w1=1.006641722791368\n",
            "Gradient Descent(211/999): loss=9.580729717901598, w0=-2.0471010325555135, w1=1.0073137013886428\n",
            "Gradient Descent(212/999): loss=9.576218490244068, w0=-2.0537658776852963, w1=1.007983257374295\n",
            "Gradient Descent(213/999): loss=9.571739731632775, w0=-2.060406694771118, w1=1.0086503994823033\n",
            "Gradient Descent(214/999): loss=9.567293208375467, w0=-2.067023570438682, w1=1.0093151364151594\n",
            "Gradient Descent(215/999): loss=9.562878688461858, w0=-2.0736165910013886, w1=1.0099774768439806\n",
            "Gradient Descent(216/999): loss=9.558495941551536, w0=-2.0801858424614634, w1=1.0106374294086233\n",
            "Gradient Descent(217/999): loss=9.554144738961934, w0=-2.086731410511076, w1=1.0112950027177954\n",
            "Gradient Descent(218/999): loss=9.549824853656403, w0=-2.09325338053346, w1=1.011950205349169\n",
            "Gradient Descent(219/999): loss=9.545536060232369, w0=-2.099751837604026, w1=1.0126030458494915\n",
            "Gradient Descent(220/999): loss=9.54127813490956, w0=-2.1062268664914714, w1=1.0132535327346985\n",
            "Gradient Descent(221/999): loss=9.53705085551834, w0=-2.112678551658886, w1=1.013901674490023\n",
            "Gradient Descent(222/999): loss=9.532854001488117, w0=-2.1191069772648543, w1=1.0145474795701084\n",
            "Gradient Descent(223/999): loss=9.528687353835823, w0=-2.125512227164553, w1=1.0151909563991162\n",
            "Gradient Descent(224/999): loss=9.5245506951545, w0=-2.1318943849108445, w1=1.0158321133708377\n",
            "Gradient Descent(225/999): loss=9.520443809601957, w0=-2.138253533755367, w1=1.0164709588488032\n",
            "Gradient Descent(226/999): loss=9.516366482889495, w0=-2.1445897566496215, w1=1.0171075011663908\n",
            "Gradient Descent(227/999): loss=9.512318502270734, w0=-2.1509031362460513, w1=1.0177417486269347\n",
            "Gradient Descent(228/999): loss=9.508299656530514, w0=-2.1571937548991236, w1=1.0183737095038345\n",
            "Gradient Descent(229/999): loss=9.504309735973873, w0=-2.163461694666401, w1=1.0190033920406623\n",
            "Gradient Descent(230/999): loss=9.500348532415103, w0=-2.169707037309613, w1=1.0196308044512712\n",
            "Gradient Descent(231/999): loss=9.496415839166886, w0=-2.1759298642957225, w1=1.0202559549199006\n",
            "Gradient Descent(232/999): loss=9.492511451029522, w0=-2.1821302567979886, w1=1.0208788516012852\n",
            "Gradient Descent(233/999): loss=9.488635164280199, w0=-2.1883082956970243, w1=1.0214995026207598\n",
            "Gradient Descent(234/999): loss=9.484786776662387, w0=-2.1944640615818534, w1=1.0221179160743663\n",
            "Gradient Descent(235/999): loss=9.480966087375268, w0=-2.200597634750961, w1=1.0227341000289578\n",
            "Gradient Descent(236/999): loss=9.477172897063273, w0=-2.2067090952133395, w1=1.0233480625223061\n",
            "Gradient Descent(237/999): loss=9.473407007805662, w0=-2.212798522689535, w1=1.0239598115632045\n",
            "Gradient Descent(238/999): loss=9.469668223106215, w0=-2.2188659966126854, w1=1.0245693551315733\n",
            "Gradient Descent(239/999): loss=9.465956347882967, w0=-2.224911596129556, w1=1.0251767011785635\n",
            "Gradient Descent(240/999): loss=9.46227118845803, w0=-2.2309354001015738, w1=1.025781857626661\n",
            "Gradient Descent(241/999): loss=9.458612552547502, w0=-2.236937487105855, w1=1.0263848323697893\n",
            "Gradient Descent(242/999): loss=9.454980249251408, w0=-2.24291793543623, w1=1.026985633273413\n",
            "Gradient Descent(243/999): loss=9.451374089043764, w0=-2.2488768231042657, w1=1.02758426817464\n",
            "Gradient Descent(244/999): loss=9.447793883762673, w0=-2.254814227840281, w1=1.0281807448823244\n",
            "Gradient Descent(245/999): loss=9.444239446600516, w0=-2.2607302270943634, w1=1.0287750711771668\n",
            "Gradient Descent(246/999): loss=9.440710592094195, w0=-2.266624898037377, w1=1.0293672548118185\n",
            "Gradient Descent(247/999): loss=9.437207136115466, w0=-2.272498317561971, w1=1.0299573035109792\n",
            "Gradient Descent(248/999): loss=9.433728895861321, w0=-2.2783505622835816, w1=1.0305452249715015\n",
            "Gradient Descent(249/999): loss=9.430275689844464, w0=-2.284181708541431, w1=1.0311310268624874\n",
            "Gradient Descent(250/999): loss=9.42684733788383, w0=-2.289991832399525, w1=1.0317147168253917\n",
            "Gradient Descent(251/999): loss=9.423443661095185, w0=-2.2957810096476434, w1=1.03229630247412\n",
            "Gradient Descent(252/999): loss=9.42006448188179, w0=-2.3015493158023292, w1=1.032875791395128\n",
            "Gradient Descent(253/999): loss=9.416709623925145, w0=-2.307296826107874, w1=1.033453191147521\n",
            "Gradient Descent(254/999): loss=9.413378912175782, w0=-2.3130236155372996, w1=1.0340285092631523\n",
            "Gradient Descent(255/999): loss=9.410072172844135, w0=-2.318729758793335, w1=1.0346017532467213\n",
            "Gradient Descent(256/999): loss=9.406789233391452, w0=-2.3244153303093924, w1=1.0351729305758715\n",
            "Gradient Descent(257/999): loss=9.403529922520825, w0=-2.3300804042505368, w1=1.035742048701288\n",
            "Gradient Descent(258/999): loss=9.400294070168233, w0=-2.3357250545144534, w1=1.0363091150467956\n",
            "Gradient Descent(259/999): loss=9.397081507493667, w0=-2.3413493547324133, w1=1.036874137009453\n",
            "Gradient Descent(260/999): loss=9.393892066872331, w0=-2.346953378270232, w1=1.0374371219596532\n",
            "Gradient Descent(261/999): loss=9.390725581885883, w0=-2.352537198229227, w1=1.0379980772412156\n",
            "Gradient Descent(262/999): loss=9.38758188731376, w0=-2.358100887447172, w1=1.0385570101714854\n",
            "Gradient Descent(263/999): loss=9.384460819124554, w0=-2.3636445184992465, w1=1.0391139280414259\n",
            "Gradient Descent(264/999): loss=9.381362214467462, w0=-2.369168163698982, w1=1.0396688381157164\n",
            "Gradient Descent(265/999): loss=9.378285911663772, w0=-2.374671895099207, w1=1.0402217476328444\n",
            "Gradient Descent(266/999): loss=9.375231750198438, w0=-2.3801557844929846, w1=1.0407726638052022\n",
            "Gradient Descent(267/999): loss=9.372199570711702, w0=-2.385619903414551, w1=1.0413215938191795\n",
            "Gradient Descent(268/999): loss=9.36918921499079, w0=-2.3910643231402466, w1=1.0418685448352583\n",
            "Gradient Descent(269/999): loss=9.366200525961629, w0=-2.3964891146894485, w1=1.042413523988105\n",
            "Gradient Descent(270/999): loss=9.363233347680683, w0=-2.4018943488254942, w1=1.0429565383866641\n",
            "Gradient Descent(271/999): loss=9.360287525326795, w0=-2.4072800960566068, w1=1.0434975951142522\n",
            "Gradient Descent(272/999): loss=9.357362905193117, w0=-2.412646426636812, w1=1.0440367012286473\n",
            "Gradient Descent(273/999): loss=9.35445933467909, w0=-2.4179934105668583, w1=1.0445738637621849\n",
            "Gradient Descent(274/999): loss=9.351576662282477, w0=-2.423321117595127, w1=1.0451090897218454\n",
            "Gradient Descent(275/999): loss=9.348714737591463, w0=-2.428629617218543, w1=1.0456423860893493\n",
            "Gradient Descent(276/999): loss=9.345873411276806, w0=-2.4339189786834816, w1=1.0461737598212464\n",
            "Gradient Descent(277/999): loss=9.343052535084043, w0=-2.4391892709866725, w1=1.0467032178490057\n",
            "Gradient Descent(278/999): loss=9.340251961825752, w0=-2.4444405628760975, w1=1.0472307670791083\n",
            "Gradient Descent(279/999): loss=9.337471545373885, w0=-2.4496729228518896, w1=1.0477564143931357\n",
            "Gradient Descent(280/999): loss=9.334711140652121, w0=-2.4548864191672264, w1=1.0482801666478594\n",
            "Gradient Descent(281/999): loss=9.331970603628324, w0=-2.4600811198292183, w1=1.0488020306753318\n",
            "Gradient Descent(282/999): loss=9.329249791307, w0=-2.4652570925997974, w1=1.0493220132829735\n",
            "Gradient Descent(283/999): loss=9.326548561721854, w0=-2.470414404996602, w1=1.0498401212536639\n",
            "Gradient Descent(284/999): loss=9.32386677392838, w0=-2.475553124293855, w1=1.0503563613458284\n",
            "Gradient Descent(285/999): loss=9.3212042879965, w0=-2.4806733175232436, w1=1.0508707402935273\n",
            "Gradient Descent(286/999): loss=9.318560965003266, w0=-2.485775051474793, w1=1.0513832648065424\n",
            "Gradient Descent(287/999): loss=9.315936667025618, w0=-2.490858392697738, w1=1.0518939415704667\n",
            "Gradient Descent(288/999): loss=9.313331257133173, w0=-2.4959234075013894, w1=1.0524027772467899\n",
            "Gradient Descent(289/999): loss=9.310744599381097, w0=-2.5009701619560007, w1=1.052909778472985\n",
            "Gradient Descent(290/999): loss=9.308176558802995, w0=-2.50599872189363, w1=1.0534149518625968\n",
            "Gradient Descent(291/999): loss=9.305627001403888, w0=-2.511009152908998, w1=1.0539183040053262\n",
            "Gradient Descent(292/999): loss=9.3030957941532, w0=-2.5160015203603434, w1=1.0544198414671175\n",
            "Gradient Descent(293/999): loss=9.300582804977836, w0=-2.5209758893702765, w1=1.054919570790243\n",
            "Gradient Descent(294/999): loss=9.298087902755276, w0=-2.5259323248266274, w1=1.0554174984933893\n",
            "Gradient Descent(295/999): loss=9.295610957306737, w0=-2.530870891383294, w1=1.0559136310717412\n",
            "Gradient Descent(296/999): loss=9.293151839390394, w0=-2.535791653461084, w1=1.0564079749970685\n",
            "Gradient Descent(297/999): loss=9.290710420694614, w0=-2.540694675248556, w1=1.0569005367178073\n",
            "Gradient Descent(298/999): loss=9.288286573831279, w0=-2.5455800207028565, w1=1.0573913226591471\n",
            "Gradient Descent(299/999): loss=9.285880172329131, w0=-2.5504477535505536, w1=1.057880339223113\n",
            "Gradient Descent(300/999): loss=9.283491090627168, w0=-2.55529793728847, w1=1.0583675927886493\n",
            "Gradient Descent(301/999): loss=9.281119204068114, w0=-2.560130635184509, w1=1.0588530897117032\n",
            "Gradient Descent(302/999): loss=9.278764388891885, w0=-2.564945910278482, w1=1.059336836325308\n",
            "Gradient Descent(303/999): loss=9.276426522229148, w0=-2.5697438253829294, w1=1.0598188389396637\n",
            "Gradient Descent(304/999): loss=9.27410548209492, w0=-2.5745244430839405, w1=1.060299103842223\n",
            "Gradient Descent(305/999): loss=9.271801147382174, w0=-2.5792878257419694, w1=1.0607776372977689\n",
            "Gradient Descent(306/999): loss=9.269513397855546, w0=-2.5840340354926488, w1=1.0612544455485005\n",
            "Gradient Descent(307/999): loss=9.267242114145057, w0=-2.588763134247601, w1=1.061729534814111\n",
            "Gradient Descent(308/999): loss=9.264987177739872, w0=-2.593475183695245, w1=1.062202911291872\n",
            "Gradient Descent(309/999): loss=9.262748470982126, w0=-2.5981702453016005, w1=1.0626745811567113\n",
            "Gradient Descent(310/999): loss=9.260525877060788, w0=-2.6028483803110913, w1=1.0631445505612962\n",
            "Gradient Descent(311/999): loss=9.25831928000555, w0=-2.607509649747343, w1=1.063612825636111\n",
            "Gradient Descent(312/999): loss=9.256128564680797, w0=-2.612154114413979, w1=1.0640794124895399\n",
            "Gradient Descent(313/999): loss=9.253953616779587, w0=-2.6167818348954146, w1=1.064544317207944\n",
            "Gradient Descent(314/999): loss=9.251794322817684, w0=-2.621392871557646, w1=1.065007545855743\n",
            "Gradient Descent(315/999): loss=9.249650570127653, w0=-2.6259872845490393, w1=1.065469104475492\n",
            "Gradient Descent(316/999): loss=9.24752224685296, w0=-2.630565133801113, w1=1.065928999087963\n",
            "Gradient Descent(317/999): loss=9.245409241942147, w0=-2.635126479029322, w1=1.0663872356922202\n",
            "Gradient Descent(318/999): loss=9.243311445143041, w0=-2.6396713797338354, w1=1.0668438202657016\n",
            "Gradient Descent(319/999): loss=9.241228746996994, w0=-2.644199895200312, w1=1.0672987587642944\n",
            "Gradient Descent(320/999): loss=9.23916103883317, w0=-2.648712084500676, w1=1.0677520571224144\n",
            "Gradient Descent(321/999): loss=9.23710821276289, w0=-2.6532080064938843, w1=1.068203721253082\n",
            "Gradient Descent(322/999): loss=9.235070161673972, w0=-2.6576877198266966, w1=1.0686537570480001\n",
            "Gradient Descent(323/999): loss=9.233046779225186, w0=-2.6621512829344405, w1=1.069102170377632\n",
            "Gradient Descent(324/999): loss=9.231037959840661, w0=-2.6665987540417717, w1=1.0695489670912752\n",
            "Gradient Descent(325/999): loss=9.229043598704408, w0=-2.671030191163436, w1=1.0699941530171404\n",
            "Gradient Descent(326/999): loss=9.227063591754835, w0=-2.675445652105025, w1=1.0704377339624265\n",
            "Gradient Descent(327/999): loss=9.225097835679318, w0=-2.679845194463729, w1=1.0708797157133956\n",
            "Gradient Descent(328/999): loss=9.223146227908819, w0=-2.6842288756290897, w1=1.07132010403545\n",
            "Gradient Descent(329/999): loss=9.221208666612528, w0=-2.6885967527837495, w1=1.0717589046732063\n",
            "Gradient Descent(330/999): loss=9.219285050692552, w0=-2.6929488829041954, w1=1.0721961233505706\n",
            "Gradient Descent(331/999): loss=9.217375279778631, w0=-2.6972853227615032, w1=1.072631765770813\n",
            "Gradient Descent(332/999): loss=9.215479254222917, w0=-2.7016061289220787, w1=1.073065837616643\n",
            "Gradient Descent(333/999): loss=9.213596875094764, w0=-2.705911357748395, w1=1.0734983445502821\n",
            "Gradient Descent(334/999): loss=9.211728044175565, w0=-2.710201065399727, w1=1.0739292922135386\n",
            "Gradient Descent(335/999): loss=9.209872663953636, w0=-2.714475307832885, w1=1.0743586862278813\n",
            "Gradient Descent(336/999): loss=9.208030637619112, w0=-2.718734140802945, w1=1.074786532194512\n",
            "Gradient Descent(337/999): loss=9.206201869058916, w0=-2.7229776198639737, w1=1.0752128356944395\n",
            "Gradient Descent(338/999): loss=9.204386262851731, w0=-2.7272058003697563, w1=1.0756376022885519\n",
            "Gradient Descent(339/999): loss=9.202583724263024, w0=-2.731418737474516, w1=1.0760608375176888\n",
            "Gradient Descent(340/999): loss=9.200794159240099, w0=-2.7356164861336345, w1=1.0764825469027153\n",
            "Gradient Descent(341/999): loss=9.199017474407196, w0=-2.7397991011043694, w1=1.0769027359445908\n",
            "Gradient Descent(342/999): loss=9.197253577060625, w0=-2.743966636946568, w1=1.0773214101244446\n",
            "Gradient Descent(343/999): loss=9.195502375163905, w0=-2.7481191480233775, w1=1.0777385749036439\n",
            "Gradient Descent(344/999): loss=9.193763777342985, w0=-2.7522566885019573, w1=1.0781542357238678\n",
            "Gradient Descent(345/999): loss=9.192037692881472, w0=-2.7563793123541824, w1=1.0785683980071765\n",
            "Gradient Descent(346/999): loss=9.190324031715885, w0=-2.7604870733573503, w1=1.0789810671560827\n",
            "Gradient Descent(347/999): loss=9.188622704430973, w0=-2.7645800250948795, w1=1.079392248553622\n",
            "Gradient Descent(348/999): loss=9.186933622255037, w0=-2.768658220957011, w1=1.0798019475634235\n",
            "Gradient Descent(349/999): loss=9.1852566970553, w0=-2.7727217141415035, w1=1.0802101695297788\n",
            "Gradient Descent(350/999): loss=9.183591841333314, w0=-2.7767705576543276, w1=1.0806169197777127\n",
            "Gradient Descent(351/999): loss=9.181938968220393, w0=-2.780804804310357, w1=1.0810222036130517\n",
            "Gradient Descent(352/999): loss=9.180297991473072, w0=-2.7848245067340573, w1=1.0814260263224953\n",
            "Gradient Descent(353/999): loss=9.178668825468622, w0=-2.788829717360174, w1=1.0818283931736812\n",
            "Gradient Descent(354/999): loss=9.177051385200562, w0=-2.792820488434415, w1=1.0822293094152582\n",
            "Gradient Descent(355/999): loss=9.17544558627425, w0=-2.796796872014131, w1=1.0826287802769516\n",
            "Gradient Descent(356/999): loss=9.173851344902452, w0=-2.800758919968998, w1=1.083026810969633\n",
            "Gradient Descent(357/999): loss=9.172268577900992, w0=-2.8047066839816903, w1=1.0834234066853876\n",
            "Gradient Descent(358/999): loss=9.170697202684396, w0=-2.8086402155485573, w1=1.0838185725975824\n",
            "Gradient Descent(359/999): loss=9.169137137261593, w0=-2.8125595659802936, w1=1.084212313860933\n",
            "Gradient Descent(360/999): loss=9.167588300231635, w0=-2.8164647864026087, w1=1.0846046356115715\n",
            "Gradient Descent(361/999): loss=9.166050610779443, w0=-2.8203559277568946, w1=1.0849955429671136\n",
            "Gradient Descent(362/999): loss=9.1645239886716, w0=-2.8242330408008898, w1=1.0853850410267243\n",
            "Gradient Descent(363/999): loss=9.163008354252161, w0=-2.8280961761093413, w1=1.0857731348711859\n",
            "Gradient Descent(364/999): loss=9.161503628438483, w0=-2.8319453840746647, w1=1.0861598295629624\n",
            "Gradient Descent(365/999): loss=9.160009732717132, w0=-2.835780714907601, w1=1.0865451301462685\n",
            "Gradient Descent(366/999): loss=9.158526589139749, w0=-2.8396022186378715, w1=1.0869290416471313\n",
            "Gradient Descent(367/999): loss=9.157054120319003, w0=-2.8434099451148316, w1=1.08731156907346\n",
            "Gradient Descent(368/999): loss=9.155592249424558, w0=-2.8472039440081196, w1=1.087692717415108\n",
            "Gradient Descent(369/999): loss=9.15414090017904, w0=-2.8509842648083055, w1=1.0880724916439404\n",
            "Gradient Descent(370/999): loss=9.15269999685409, w0=-2.854750956827536, w1=1.0884508967138966\n",
            "Gradient Descent(371/999): loss=9.151269464266383, w0=-2.8585040692001784, w1=1.0888279375610574\n",
            "Gradient Descent(372/999): loss=9.14984922777372, w0=-2.862243650883461, w1=1.0892036191037067\n",
            "Gradient Descent(373/999): loss=9.148439213271132, w0=-2.8659697506581128, w1=1.0895779462423985\n",
            "Gradient Descent(374/999): loss=9.147039347187016, w0=-2.869682417128997, w1=1.0899509238600182\n",
            "Gradient Descent(375/999): loss=9.145649556479285, w0=-2.873381698725749, w1=1.0903225568218478\n",
            "Gradient Descent(376/999): loss=9.144269768631567, w0=-2.8770676437034046, w1=1.0906928499756294\n",
            "Gradient Descent(377/999): loss=9.14289991164942, w0=-2.8807403001430316, w1=1.0910618081516277\n",
            "Gradient Descent(378/999): loss=9.141539914056573, w0=-2.8843997159523562, w1=1.0914294361626935\n",
            "Gradient Descent(379/999): loss=9.140189704891194, w0=-2.888045938866388, w1=1.091795738804326\n",
            "Gradient Descent(380/999): loss=9.138849213702194, w0=-2.8916790164480433, w1=1.0921607208547364\n",
            "Gradient Descent(381/999): loss=9.13751837054555, w0=-2.8952989960887643, w1=1.0925243870749088\n",
            "Gradient Descent(382/999): loss=9.136197105980647, w0=-2.898905925009138, w1=1.0928867422086637\n",
            "Gradient Descent(383/999): loss=9.134885351066664, w0=-2.9024998502595123, w1=1.0932477909827187\n",
            "Gradient Descent(384/999): loss=9.133583037358976, w0=-2.90608081872061, w1=1.093607538106751\n",
            "Gradient Descent(385/999): loss=9.132290096905576, w0=-2.9096488771041393, w1=1.093965988273458\n",
            "Gradient Descent(386/999): loss=9.131006462243539, w0=-2.913204071953404, w1=1.0943231461586198\n",
            "Gradient Descent(387/999): loss=9.129732066395492, w0=-2.9167464496439104, w1=1.094679016421159\n",
            "Gradient Descent(388/999): loss=9.128466842866123, w0=-2.920276056383972, w1=1.0950336037032018\n",
            "Gradient Descent(389/999): loss=9.127210725638717, w0=-2.9237929382153123, w1=1.095386912630139\n",
            "Gradient Descent(390/999): loss=9.125963649171705, w0=-2.9272971410136663, w1=1.0957389478106851\n",
            "Gradient Descent(391/999): loss=9.124725548395249, w0=-2.9307887104893777, w1=1.0960897138369408\n",
            "Gradient Descent(392/999): loss=9.123496358707834, w0=-2.9342676921879955, w1=1.09643921528445\n",
            "Gradient Descent(393/999): loss=9.122276015972918, w0=-2.9377341314908687, w1=1.0967874567122615\n",
            "Gradient Descent(394/999): loss=9.121064456515574, w0=-2.941188073615738, w1=1.0971344426629879\n",
            "Gradient Descent(395/999): loss=9.119861617119154, w0=-2.9446295636173243, w1=1.0974801776628644\n",
            "Gradient Descent(396/999): loss=9.118667435022022, w0=-2.948058646387919, w1=1.0978246662218092\n",
            "Gradient Descent(397/999): loss=9.117481847914252, w0=-2.951475366657967, w1=1.09816791283348\n",
            "Gradient Descent(398/999): loss=9.11630479393439, w0=-2.9548797689966526, w1=1.0985099219753351\n",
            "Gradient Descent(399/999): loss=9.115136211666217, w0=-2.9582718978124785, w1=1.0988506981086905\n",
            "Gradient Descent(400/999): loss=9.113976040135562, w0=-2.9616517973538468, w1=1.0991902456787779\n",
            "Gradient Descent(401/999): loss=9.112824218807098, w0=-2.965019511709636, w1=1.0995285691148031\n",
            "Gradient Descent(402/999): loss=9.111680687581195, w0=-2.9683750848097747, w1=1.0998656728300045\n",
            "Gradient Descent(403/999): loss=9.110545386790793, w0=-2.971718560425817, w1=1.100201561221709\n",
            "Gradient Descent(404/999): loss=9.109418257198266, w0=-2.9750499821715106, w1=1.1005362386713908\n",
            "Gradient Descent(405/999): loss=9.108299239992348, w0=-2.978369393503369, w1=1.1008697095447284\n",
            "Gradient Descent(406/999): loss=9.107188276785072, w0=-2.9816768377212353, w1=1.10120197819166\n",
            "Gradient Descent(407/999): loss=9.106085309608693, w0=-2.9849723579688487, w1=1.1015330489464428\n",
            "Gradient Descent(408/999): loss=9.104990280912691, w0=-2.9882559972344076, w1=1.101862926127707\n",
            "Gradient Descent(409/999): loss=9.103903133560767, w0=-2.991527798351129, w1=1.1021916140385148\n",
            "Gradient Descent(410/999): loss=9.102823810827836, w0=-2.994787803997808, w1=1.102519116966413\n",
            "Gradient Descent(411/999): loss=9.101752256397106, w0=-2.998036056699375, w1=1.102845439183493\n",
            "Gradient Descent(412/999): loss=9.1006884143571, w0=-3.0012725988274487, w1=1.103170584946443\n",
            "Gradient Descent(413/999): loss=9.099632229198772, w0=-3.0044974726008915, w1=1.1034945584966056\n",
            "Gradient Descent(414/999): loss=9.098583645812587, w0=-3.007710720086358, w1=1.1038173640600333\n",
            "Gradient Descent(415/999): loss=9.097542609485659, w0=-3.010912383198844, w1=1.1041390058475409\n",
            "Gradient Descent(416/999): loss=9.096509065898887, w0=-3.0141025037022344, w1=1.1044594880547647\n",
            "Gradient Descent(417/999): loss=9.095482961124132, w0=-3.0172811232098473, w1=1.104778814862213\n",
            "Gradient Descent(418/999): loss=9.094464241621393, w0=-3.0204482831849764, w1=1.1050969904353243\n",
            "Gradient Descent(419/999): loss=9.093452854236018, w0=-3.023604024941432, w1=1.105414018924518\n",
            "Gradient Descent(420/999): loss=9.092448746195926, w0=-3.0267483896440814, w1=1.1057299044652527\n",
            "Gradient Descent(421/999): loss=9.091451865108864, w0=-3.029881418309383, w1=1.1060446511780753\n",
            "Gradient Descent(422/999): loss=9.090462158959662, w0=-3.033003151805925, w1=1.1063582631686797\n",
            "Gradient Descent(423/999): loss=9.08947957610752, w0=-3.036113630854955, w1=1.1066707445279562\n",
            "Gradient Descent(424/999): loss=9.088504065283319, w0=-3.039212896030912, w1=1.106982099332048\n",
            "Gradient Descent(425/999): loss=9.087535575586953, w0=-3.042300987761959, w1=1.1072923316424022\n",
            "Gradient Descent(426/999): loss=9.08657405648465, w0=-3.0453779463305053, w1=1.1076014455058236\n",
            "Gradient Descent(427/999): loss=9.085619457806356, w0=-3.0484438118737356, w1=1.1079094449545281\n",
            "Gradient Descent(428/999): loss=9.084671729743109, w0=-3.0514986243841324, w1=1.1082163340061943\n",
            "Gradient Descent(429/999): loss=9.083730822844439, w0=-3.054542423709997, w1=1.1085221166640162\n",
            "Gradient Descent(430/999): loss=9.082796688015799, w0=-3.05757524955597, w1=1.1088267969167558\n",
            "Gradient Descent(431/999): loss=9.08186927651598, w0=-3.0605971414835498, w1=1.1091303787387943\n",
            "Gradient Descent(432/999): loss=9.08094853995459, w0=-3.063608138911607, w1=1.1094328660901853\n",
            "Gradient Descent(433/999): loss=9.08003443028953, w0=-3.066608281116901, w1=1.1097342629167048\n",
            "Gradient Descent(434/999): loss=9.079126899824464, w0=-3.0695976072345896, w1=1.1100345731499046\n",
            "Gradient Descent(435/999): loss=9.078225901206348, w0=-3.0725761562587417, w1=1.1103338007071613\n",
            "Gradient Descent(436/999): loss=9.077331387422964, w0=-3.075543967042845, w1=1.1106319494917294\n",
            "Gradient Descent(437/999): loss=9.076443311800453, w0=-3.0785010783003126, w1=1.1109290233927909\n",
            "Gradient Descent(438/999): loss=9.075561628000889, w0=-3.0814475286049885, w1=1.1112250262855075\n",
            "Gradient Descent(439/999): loss=9.074686290019851, w0=-3.0843833563916507, w1=1.1115199620310685\n",
            "Gradient Descent(440/999): loss=9.073817252184035, w0=-3.087308599956512, w1=1.1118138344767452\n",
            "Gradient Descent(441/999): loss=9.07295446914887, w0=-3.090223297457721, w1=1.112106647455936\n",
            "Gradient Descent(442/999): loss=9.072097895896137, w0=-3.093127486915858, w1=1.1123984047882218\n",
            "Gradient Descent(443/999): loss=9.071247487731634, w0=-3.0960212062144317, w1=1.1126891102794116\n",
            "Gradient Descent(444/999): loss=9.070403200282849, w0=-3.098904493100374, w1=1.1129787677215939\n",
            "Gradient Descent(445/999): loss=9.069564989496623, w0=-3.101777385184532, w1=1.1132673808931868\n",
            "Gradient Descent(446/999): loss=9.068732811636874, w0=-3.104639919942158, w1=1.113554953558985\n",
            "Gradient Descent(447/999): loss=9.067906623282301, w0=-3.1074921347133992, w1=1.1138414894702122\n",
            "Gradient Descent(448/999): loss=9.067086381324126, w0=-3.110334066703784, w1=1.114126992364567\n",
            "Gradient Descent(449/999): loss=9.066272042963833, w0=-3.1131657529847088, w1=1.1144114659662727\n",
            "Gradient Descent(450/999): loss=9.065463565710958, w0=-3.1159872304939187, w1=1.1146949139861273\n",
            "Gradient Descent(451/999): loss=9.064660907380844, w0=-3.1187985360359924, w1=1.1149773401215497\n",
            "Gradient Descent(452/999): loss=9.063864026092464, w0=-3.1215997062828214, w1=1.1152587480566287\n",
            "Gradient Descent(453/999): loss=9.063072880266212, w0=-3.1243907777740865, w1=1.1155391414621723\n",
            "Gradient Descent(454/999): loss=9.062287428621758, w0=-3.127171786917738, w1=1.1158185239957532\n",
            "Gradient Descent(455/999): loss=9.06150763017588, w0=-3.1299427699904663, w1=1.1160968993017595\n",
            "Gradient Descent(456/999): loss=9.060733444240325, w0=-3.132703763138179, w1=1.1163742710114384\n",
            "Gradient Descent(457/999): loss=9.059964830419688, w0=-3.1354548023764703, w1=1.1166506427429483\n",
            "Gradient Descent(458/999): loss=9.05920174860931, w0=-3.1381959235910912, w1=1.1169260181014011\n",
            "Gradient Descent(459/999): loss=9.058444158993177, w0=-3.140927162538418, w1=1.1172004006789131\n",
            "Gradient Descent(460/999): loss=9.057692022041852, w0=-3.1436485548459174, w1=1.11747379405465\n",
            "Gradient Descent(461/999): loss=9.056945298510394, w0=-3.146360136012614, w1=1.1177462017948734\n",
            "Gradient Descent(462/999): loss=9.056203949436341, w0=-3.14906194140955, w1=1.1180176274529883\n",
            "Gradient Descent(463/999): loss=9.055467936137642, w0=-3.151754006280249, w1=1.118288074569589\n",
            "Gradient Descent(464/999): loss=9.054737220210667, w0=-3.154436365741175, w1=1.1185575466725053\n",
            "Gradient Descent(465/999): loss=9.054011763528186, w0=-3.15710905478219, w1=1.118826047276848\n",
            "Gradient Descent(466/999): loss=9.053291528237386, w0=-3.1597721082670107, w1=1.119093579885056\n",
            "Gradient Descent(467/999): loss=9.052576476757897, w0=-3.1624255609336642, w1=1.1193601479869408\n",
            "Gradient Descent(468/999): loss=9.05186657177983, w0=-3.16506944739494, w1=1.1196257550597326\n",
            "Gradient Descent(469/999): loss=9.051161776261825, w0=-3.1677038021388415, w1=1.119890404568125\n",
            "Gradient Descent(470/999): loss=9.05046205342913, w0=-3.1703286595290368, w1=1.120154099964322\n",
            "Gradient Descent(471/999): loss=9.04976736677167, w0=-3.1729440538053058, w1=1.1204168446880802\n",
            "Gradient Descent(472/999): loss=9.049077680042148, w0=-3.1755500190839876, w1=1.120678642166756\n",
            "Gradient Descent(473/999): loss=9.048392957254155, w0=-3.178146589358426, w1=1.12093949581535\n",
            "Gradient Descent(474/999): loss=9.047713162680285, w0=-3.1807337984994115, w1=1.1211994090365498\n",
            "Gradient Descent(475/999): loss=9.047038260850284, w0=-3.183311680255624, w1=1.121458385220777\n",
            "Gradient Descent(476/999): loss=9.04636821654918, w0=-3.1858802682540737, w1=1.1217164277462288\n",
            "Gradient Descent(477/999): loss=9.045702994815468, w0=-3.188439596000538, w1=1.1219735399789248\n",
            "Gradient Descent(478/999): loss=9.045042560939265, w0=-3.19098969688, w1=1.1222297252727482\n",
            "Gradient Descent(479/999): loss=9.04438688046052, w0=-3.1935306041570835, w1=1.122484986969491\n",
            "Gradient Descent(480/999): loss=9.043735919167194, w0=-3.196062350976487, w1=1.1227393283988985\n",
            "Gradient Descent(481/999): loss=9.043089643093486, w0=-3.198584970363416, w1=1.12299275287871\n",
            "Gradient Descent(482/999): loss=9.042448018518067, w0=-3.201098495224014, w1=1.123245263714705\n",
            "Gradient Descent(483/999): loss=9.041811011962308, w0=-3.203602958345791, w1=1.1234968642007443\n",
            "Gradient Descent(484/999): loss=9.041178590188531, w0=-3.2060983923980517, w1=1.1237475576188143\n",
            "Gradient Descent(485/999): loss=9.040550720198304, w0=-3.208584829932323, w1=1.1239973472390683\n",
            "Gradient Descent(486/999): loss=9.039927369230677, w0=-3.211062303382776, w1=1.1242462363198713\n",
            "Gradient Descent(487/999): loss=9.039308504760498, w0=-3.2135308450666504, w1=1.1244942281078398\n",
            "Gradient Descent(488/999): loss=9.03869409449672, w0=-3.2159904871846767, w1=1.1247413258378873\n",
            "Gradient Descent(489/999): loss=9.038084106380698, w0=-3.218441261821495, w1=1.1249875327332635\n",
            "Gradient Descent(490/999): loss=9.03747850858453, w0=-3.220883200946075, w1=1.1252328520055979\n",
            "Gradient Descent(491/999): loss=9.036877269509386, w0=-3.223316336412131, w1=1.125477286854942\n",
            "Gradient Descent(492/999): loss=9.036280357783872, w0=-3.2257406999585396, w1=1.1257208404698098\n",
            "Gradient Descent(493/999): loss=9.03568774226238, w0=-3.228156323209752, w1=1.1259635160272214\n",
            "Gradient Descent(494/999): loss=9.035099392023474, w0=-3.2305632376762077, w1=1.126205316692741\n",
            "Gradient Descent(495/999): loss=9.034515276368271, w0=-3.2329614747547444, w1=1.1264462456205224\n",
            "Gradient Descent(496/999): loss=9.033935364818836, w0=-3.2353510657290085, w1=1.1266863059533467\n",
            "Gradient Descent(497/999): loss=9.033359627116594, w0=-3.237732041769863, w1=1.1269255008226655\n",
            "Gradient Descent(498/999): loss=9.032788033220765, w0=-3.2401044339357936, w1=1.1271638333486405\n",
            "Gradient Descent(499/999): loss=9.032220553306768, w0=-3.2424682731733148, w1=1.1274013066401845\n",
            "Gradient Descent(500/999): loss=9.031657157764688, w0=-3.244823590317372, w1=1.1276379237950027\n",
            "Gradient Descent(501/999): loss=9.031097817197729, w0=-3.247170416091746, w1=1.1278736878996316\n",
            "Gradient Descent(502/999): loss=9.030542502420667, w0=-3.2495087811094514, w1=1.1281086020294813\n",
            "Gradient Descent(503/999): loss=9.029991184458341, w0=-3.251838715873138, w1=1.1283426692488727\n",
            "Gradient Descent(504/999): loss=9.02944383454413, w0=-3.2541602507754863, w1=1.1285758926110816\n",
            "Gradient Descent(505/999): loss=9.028900424118463, w0=-3.256473416099607, w1=1.1288082751583737\n",
            "Gradient Descent(506/999): loss=9.028360924827318, w0=-3.258778242019433, w1=1.1290398199220488\n",
            "Gradient Descent(507/999): loss=9.027825308520754, w0=-3.261074758600115, w1=1.1292705299224768\n",
            "Gradient Descent(508/999): loss=9.02729354725143, w0=-3.2633629957984134, w1=1.1295004081691393\n",
            "Gradient Descent(509/999): loss=9.02676561327316, w0=-3.265642983463088, w1=1.1297294576606682\n",
            "Gradient Descent(510/999): loss=9.026241479039447, w0=-3.267914751335289, w1=1.1299576813848846\n",
            "Gradient Descent(511/999): loss=9.025721117202067, w0=-3.270178329048943, w1=1.1301850823188377\n",
            "Gradient Descent(512/999): loss=9.025204500609627, w0=-3.2724337461311412, w1=1.1304116634288444\n",
            "Gradient Descent(513/999): loss=9.024691602306154, w0=-3.2746810320025244, w1=1.1306374276705267\n",
            "Gradient Descent(514/999): loss=9.024182395529689, w0=-3.276920215977665, w1=1.1308623779888518\n",
            "Gradient Descent(515/999): loss=9.023676853710882, w0=-3.2791513272654527, w1=1.131086517318169\n",
            "Gradient Descent(516/999): loss=9.023174950471624, w0=-3.281374394969472, w1=1.1313098485822497\n",
            "Gradient Descent(517/999): loss=9.022676659623654, w0=-3.2835894480883834, w1=1.1315323746943236\n",
            "Gradient Descent(518/999): loss=9.022181955167197, w0=-3.285796515516303, w1=1.1317540985571177\n",
            "Gradient Descent(519/999): loss=9.02169081128961, w0=-3.2879956260431764, w1=1.1319750230628949\n",
            "Gradient Descent(520/999): loss=9.02120320236403, w0=-3.2901868083551573, w1=1.1321951510934902\n",
            "Gradient Descent(521/999): loss=9.020719102948044, w0=-3.2923700910349796, w1=1.1324144855203493\n",
            "Gradient Descent(522/999): loss=9.020238487782358, w0=-3.294545502562331, w1=1.1326330292045657\n",
            "Gradient Descent(523/999): loss=9.01976133178948, w0=-3.296713071314225, w1=1.1328507849969183\n",
            "Gradient Descent(524/999): loss=9.019287610072409, w0=-3.29887282556537, w1=1.133067755737908\n",
            "Gradient Descent(525/999): loss=9.01881729791333, w0=-3.3010247934885384, w1=1.1332839442577958\n",
            "Gradient Descent(526/999): loss=9.018350370772346, w0=-3.303169003154935, w1=1.1334993533766384\n",
            "Gradient Descent(527/999): loss=9.017886804286174, w0=-3.3053054825345622, w1=1.133713985904326\n",
            "Gradient Descent(528/999): loss=9.017426574266882, w0=-3.3074342594965853, w1=1.1339278446406187\n",
            "Gradient Descent(529/999): loss=9.01696965670063, w0=-3.309555361809696, w1=1.134140932375182\n",
            "Gradient Descent(530/999): loss=9.01651602774642, w0=-3.3116688171424746, w1=1.1343532518876265\n",
            "Gradient Descent(531/999): loss=9.016065663734837, w0=-3.31377465306375, w1=1.1345648059475384\n",
            "Gradient Descent(532/999): loss=9.015618541166832, w0=-3.315872897042962, w1=1.1347755973145217\n",
            "Gradient Descent(533/999): loss=9.015174636712484, w0=-3.3179635764505155, w1=1.1349856287382307\n",
            "Gradient Descent(534/999): loss=9.014733927209786, w0=-3.3200467185581415, w1=1.1351949029584059\n",
            "Gradient Descent(535/999): loss=9.01429638966344, w0=-3.3221223505392508, w1=1.135403422704912\n",
            "Gradient Descent(536/999): loss=9.013862001243652, w0=-3.324190499469289, w1=1.13561119069777\n",
            "Gradient Descent(537/999): loss=9.013430739284942, w0=-3.3262511923260885, w1=1.1358182096471972\n",
            "Gradient Descent(538/999): loss=9.013002581284962, w0=-3.3283044559902226, w1=1.1360244822536374\n",
            "Gradient Descent(539/999): loss=9.012577504903328, w0=-3.330350317245355, w1=1.1362300112078014\n",
            "Gradient Descent(540/999): loss=9.012155487960438, w0=-3.3323888027785884, w1=1.136434799190697\n",
            "Gradient Descent(541/999): loss=9.011736508436334, w0=-3.3344199391808136, w1=1.1366388488736687\n",
            "Gradient Descent(542/999): loss=9.01132054446954, w0=-3.3364437529470568, w1=1.1368421629184282\n",
            "Gradient Descent(543/999): loss=9.010907574355926, w0=-3.3384602704768236, w1=1.137044743977093\n",
            "Gradient Descent(544/999): loss=9.010497576547575, w0=-3.340469518074445, w1=1.137246594692218\n",
            "Gradient Descent(545/999): loss=9.010090529651661, w0=-3.3424715219494194, w1=1.1374477176968318\n",
            "Gradient Descent(546/999): loss=9.00968641242933, w0=-3.3444663082167554, w1=1.1376481156144702\n",
            "Gradient Descent(547/999): loss=9.009285203794583, w0=-3.3464539028973115, w1=1.1378477910592109\n",
            "Gradient Descent(548/999): loss=9.008886882813202, w0=-3.348434331918136, w1=1.1380467466357067\n",
            "Gradient Descent(549/999): loss=9.008491428701626, w0=-3.3504076211128067, w1=1.138244984939221\n",
            "Gradient Descent(550/999): loss=9.00809882082589, w0=-3.3523737962217637, w1=1.1384425085556604\n",
            "Gradient Descent(551/999): loss=9.007709038700533, w0=-3.35433288289265, w1=1.1386393200616085\n",
            "Gradient Descent(552/999): loss=9.007322061987539, w0=-3.3562849066806435, w1=1.13883542202436\n",
            "Gradient Descent(553/999): loss=9.00693787049528, w0=-3.35822989304879, w1=1.1390308170019547\n",
            "Gradient Descent(554/999): loss=9.006556444177438, w0=-3.3601678673683377, w1=1.1392255075432085\n",
            "Gradient Descent(555/999): loss=9.006177763131989, w0=-3.3620988549190645, w1=1.1394194961877508\n",
            "Gradient Descent(556/999): loss=9.005801807600152, w0=-3.3640228808896113, w1=1.1396127854660527\n",
            "Gradient Descent(557/999): loss=9.005428557965347, w0=-3.3659399703778092, w1=1.1398053778994637\n",
            "Gradient Descent(558/999): loss=9.005057994752194, w0=-3.367850148391006, w1=1.1399972760002437\n",
            "Gradient Descent(559/999): loss=9.004690098625481, w0=-3.3697534398463937, w1=1.1401884822715942\n",
            "Gradient Descent(560/999): loss=9.004324850389152, w0=-3.3716498695713333, w1=1.1403789992076936\n",
            "Gradient Descent(561/999): loss=9.003962230985323, w0=-3.3735394623036776, w1=1.1405688292937266\n",
            "Gradient Descent(562/999): loss=9.003602221493269, w0=-3.3754222426920952, w1=1.1407579750059207\n",
            "Gradient Descent(563/999): loss=9.00324480312845, w0=-3.377298235296392, w1=1.140946438811573\n",
            "Gradient Descent(564/999): loss=9.002889957241523, w0=-3.37916746458783, w1=1.1411342231690893\n",
            "Gradient Descent(565/999): loss=9.002537665317371, w0=-3.3810299549494482, w1=1.1413213305280085\n",
            "Gradient Descent(566/999): loss=9.002187908974138, w0=-3.38288573067638, w1=1.1415077633290414\n",
            "Gradient Descent(567/999): loss=9.001840669962274, w0=-3.384734815976171, w1=1.141693524004098\n",
            "Gradient Descent(568/999): loss=9.001495930163568, w0=-3.3865772349690926, w1=1.1418786149763214\n",
            "Gradient Descent(569/999): loss=9.001153671590219, w0=-3.3884130116884585, w1=1.1420630386601185\n",
            "Gradient Descent(570/999): loss=9.000813876383898, w0=-3.3902421700809384, w1=1.142246797461192\n",
            "Gradient Descent(571/999): loss=9.000476526814793, w0=-3.3920647340068686, w1=1.142429893776572\n",
            "Gradient Descent(572/999): loss=9.000141605280712, w0=-3.393880727240565, w1=1.1426123299946456\n",
            "Gradient Descent(573/999): loss=8.999809094306153, w0=-3.3956901734706317, w1=1.1427941084951907\n",
            "Gradient Descent(574/999): loss=8.999478976541386, w0=-3.3974930963002725, w1=1.1429752316494048\n",
            "Gradient Descent(575/999): loss=8.999151234761557, w0=-3.3992895192475956, w1=1.143155701819937\n",
            "Gradient Descent(576/999): loss=8.998825851865785, w0=-3.4010794657459225, w1=1.1433355213609189\n",
            "Gradient Descent(577/999): loss=8.998502810876273, w0=-3.402862959144093, w1=1.143514692617994\n",
            "Gradient Descent(578/999): loss=8.998182094937416, w0=-3.4046400227067695, w1=1.143693217928351\n",
            "Gradient Descent(579/999): loss=8.99786368731493, w0=-3.4064106796147415, w1=1.1438710996207506\n",
            "Gradient Descent(580/999): loss=8.99754757139497, w0=-3.408174952965227, w1=1.1440483400155599\n",
            "Gradient Descent(581/999): loss=8.997233730683268, w0=-3.409932865772174, w1=1.1442249414247787\n",
            "Gradient Descent(582/999): loss=8.99692214880427, w0=-3.4116844409665608, w1=1.1444009061520732\n",
            "Gradient Descent(583/999): loss=8.996612809500292, w0=-3.4134297013966957, w1=1.1445762364928032\n",
            "Gradient Descent(584/999): loss=8.996305696630648, w0=-3.4151686698285135, w1=1.1447509347340543\n",
            "Gradient Descent(585/999): loss=8.996000794170838, w0=-3.416901368945874, w1=1.144925003154666\n",
            "Gradient Descent(586/999): loss=8.995698086211684, w0=-3.4186278213508574, w1=1.1450984440252625\n",
            "Gradient Descent(587/999): loss=8.995397556958523, w0=-3.4203480495640592, w1=1.1452712596082817\n",
            "Gradient Descent(588/999): loss=8.995099190730363, w0=-3.422062076024883, w1=1.1454434521580055\n",
            "Gradient Descent(589/999): loss=8.994802971959082, w0=-3.4237699230918355, w1=1.1456150239205878\n",
            "Gradient Descent(590/999): loss=8.994508885188601, w0=-3.425471613042815, w1=1.145785977134086\n",
            "Gradient Descent(591/999): loss=8.994216915074087, w0=-3.4271671680754054, w1=1.1459563140284872\n",
            "Gradient Descent(592/999): loss=8.993927046381156, w0=-3.4288566103071627, w1=1.1461260368257404\n",
            "Gradient Descent(593/999): loss=8.993639263985061, w0=-3.430539961775905, w1=1.1462951477397836\n",
            "Gradient Descent(594/999): loss=8.99335355286992, w0=-3.4322172444400008, w1=1.1464636489765725\n",
            "Gradient Descent(595/999): loss=8.99306989812792, w0=-3.433888480178654, w1=1.1466315427341107\n",
            "Gradient Descent(596/999): loss=8.99278828495856, w0=-3.435553690792189, w1=1.1467988312024777\n",
            "Gradient Descent(597/999): loss=8.992508698667846, w0=-3.4372128980023366, w1=1.1469655165638566\n",
            "Gradient Descent(598/999): loss=8.992231124667553, w0=-3.438866123452517, w1=1.1471316009925634\n",
            "Gradient Descent(599/999): loss=8.991955548474449, w0=-3.4405133887081214, w1=1.1472970866550762\n",
            "Gradient Descent(600/999): loss=8.991681955709547, w0=-3.442154715256793, w1=1.1474619757100615\n",
            "Gradient Descent(601/999): loss=8.991410332097347, w0=-3.443790124508708, w1=1.1476262703084035\n",
            "Gradient Descent(602/999): loss=8.991140663465098, w0=-3.4454196377968564, w1=1.147789972593233\n",
            "Gradient Descent(603/999): loss=8.990872935742054, w0=-3.447043276377317, w1=1.147953084699953\n",
            "Gradient Descent(604/999): loss=8.990607134958745, w0=-3.448661061429536, w1=1.1481156087562696\n",
            "Gradient Descent(605/999): loss=8.990343247246237, w0=-3.450273014056606, w1=1.148277546882216\n",
            "Gradient Descent(606/999): loss=8.990081258835426, w0=-3.451879155285536, w1=1.1484389011901845\n",
            "Gradient Descent(607/999): loss=8.989821156056305, w0=-3.4534795060675307, w1=1.1485996737849502\n",
            "Gradient Descent(608/999): loss=8.989562925337252, w0=-3.455074087278261, w1=1.1487598667637013\n",
            "Gradient Descent(609/999): loss=8.989306553204328, w0=-3.456662919718137, w1=1.1489194822160638\n",
            "Gradient Descent(610/999): loss=8.989052026280575, w0=-3.458246024112579, w1=1.1490785222241315\n",
            "Gradient Descent(611/999): loss=8.988799331285307, w0=-3.459823421112289, w1=1.1492369888624914\n",
            "Gradient Descent(612/999): loss=8.988548455033433, w0=-3.4613951312935187, w1=1.1493948841982509\n",
            "Gradient Descent(613/999): loss=8.988299384434749, w0=-3.4629611751583385, w1=1.1495522102910656\n",
            "Gradient Descent(614/999): loss=8.988052106493274, w0=-3.4645215731349044, w1=1.1497089691931652\n",
            "Gradient Descent(615/999): loss=8.987806608306563, w0=-3.466076345577726, w1=1.1498651629493812\n",
            "Gradient Descent(616/999): loss=8.987562877065038, w0=-3.467625512767931, w1=1.1500207935971725\n",
            "Gradient Descent(617/999): loss=8.987320900051307, w0=-3.469169094913529, w1=1.1501758631666532\n",
            "Gradient Descent(618/999): loss=8.987080664639516, w0=-3.4707071121496758, w1=1.1503303736806179\n",
            "Gradient Descent(619/999): loss=8.986842158294685, w0=-3.4722395845389364, w1=1.150484327154569\n",
            "Gradient Descent(620/999): loss=8.986605368572054, w0=-3.4737665320715467, w1=1.1506377255967424\n",
            "Gradient Descent(621/999): loss=8.986370283116427, w0=-3.4752879746656737, w1=1.1507905710081343\n",
            "Gradient Descent(622/999): loss=8.98613688966154, w0=-3.476803932167676, w1=1.150942865382526\n",
            "Gradient Descent(623/999): loss=8.98590517602941, w0=-3.4783144243523614, w1=1.1510946107065123\n",
            "Gradient Descent(624/999): loss=8.985675130129701, w0=-3.479819470923246, w1=1.1512458089595248\n",
            "Gradient Descent(625/999): loss=8.985446739959103, w0=-3.481319091512812, w1=1.1513964621138588\n",
            "Gradient Descent(626/999): loss=8.985219993600698, w0=-3.4828133056827615, w1=1.1515465721347002\n",
            "Gradient Descent(627/999): loss=8.984994879223327, w0=-3.484302132924273, w1=1.151696140980149\n",
            "Gradient Descent(628/999): loss=8.984771385080995, w0=-3.485785592658256, w1=1.151845170601246\n",
            "Gradient Descent(629/999): loss=8.98454949951225, w0=-3.487263704235604, w1=1.1519936629419985\n",
            "Gradient Descent(630/999): loss=8.984329210939563, w0=-3.4887364869374466, w1=1.1521416199394052\n",
            "Gradient Descent(631/999): loss=8.984110507868737, w0=-3.490203959975401, w1=1.1522890435234814\n",
            "Gradient Descent(632/999): loss=8.983893378888306, w0=-3.491666142491823, w1=1.1524359356172844\n",
            "Gradient Descent(633/999): loss=8.983677812668931, w0=-3.493123053560057, w1=1.1525822981369385\n",
            "Gradient Descent(634/999): loss=8.983463797962823, w0=-3.4945747121846837, w1=1.1527281329916597\n",
            "Gradient Descent(635/999): loss=8.983251323603135, w0=-3.4960211373017693, w1=1.1528734420837816\n",
            "Gradient Descent(636/999): loss=8.983040378503404, w0=-3.497462347779111, w1=1.1530182273087792\n",
            "Gradient Descent(637/999): loss=8.982830951656956, w0=-3.4988983624164844, w1=1.1531624905552933\n",
            "Gradient Descent(638/999): loss=8.982623032136331, w0=-3.5003291999458885, w1=1.1533062337051572\n",
            "Gradient Descent(639/999): loss=8.982416609092732, w0=-3.5017548790317896, w1=1.1534494586334179\n",
            "Gradient Descent(640/999): loss=8.982211671755433, w0=-3.5031754182713652, w1=1.1535921672083649\n",
            "Gradient Descent(641/999): loss=8.982008209431228, w0=-3.5045908361947467, w1=1.1537343612915496\n",
            "Gradient Descent(642/999): loss=8.981806211503882, w0=-3.50600115126526, w1=1.1538760427378143\n",
            "Gradient Descent(643/999): loss=8.981605667433566, w0=-3.507406381879667, w1=1.1540172133953128\n",
            "Gradient Descent(644/999): loss=8.981406566756313, w0=-3.5088065463684073, w1=1.154157875105537\n",
            "Gradient Descent(645/999): loss=8.981208899083459, w0=-3.5102016629958346, w1=1.1542980297033387\n",
            "Gradient Descent(646/999): loss=8.981012654101125, w0=-3.511591749960456, w1=1.1544376790169562\n",
            "Gradient Descent(647/999): loss=8.980817821569653, w0=-3.5129768253951705, w1=1.1545768248680348\n",
            "Gradient Descent(648/999): loss=8.980624391323095, w0=-3.514356907367503, w1=1.1547154690716546\n",
            "Gradient Descent(649/999): loss=8.980432353268657, w0=-3.515732013879843, w1=1.1548536134363494\n",
            "Gradient Descent(650/999): loss=8.980241697386205, w0=-3.517102162869677, w1=1.154991259764136\n",
            "Gradient Descent(651/999): loss=8.980052413727705, w0=-3.518467372209823, w1=1.1551284098505312\n",
            "Gradient Descent(652/999): loss=8.97986449241674, w0=-3.519827659708666, w1=1.155265065484582\n",
            "Gradient Descent(653/999): loss=8.979677923647962, w0=-3.521183043110387, w1=1.1554012284488824\n",
            "Gradient Descent(654/999): loss=8.979492697686608, w0=-3.5225335400951954, w1=1.1555369005196021\n",
            "Gradient Descent(655/999): loss=8.979308804867971, w0=-3.5238791682795605, w1=1.1556720834665062\n",
            "Gradient Descent(656/999): loss=8.97912623559691, w0=-3.5252199452164414, w1=1.15580677905298\n",
            "Gradient Descent(657/999): loss=8.978944980347341, w0=-3.526555888395515, w1=1.155940989036051\n",
            "Gradient Descent(658/999): loss=8.978765029661746, w0=-3.527887015243404, w1=1.156074715166413\n",
            "Gradient Descent(659/999): loss=8.978586374150677, w0=-3.529213343123906, w1=1.156207959188447\n",
            "Gradient Descent(660/999): loss=8.978409004492258, w0=-3.530534889338218, w1=1.1563407228402467\n",
            "Gradient Descent(661/999): loss=8.978232911431714, w0=-3.5318516711251626, w1=1.1564730078536383\n",
            "Gradient Descent(662/999): loss=8.978058085780878, w0=-3.533163705661414, w1=1.156604815954205\n",
            "Gradient Descent(663/999): loss=8.977884518417714, w0=-3.5344710100617203, w1=1.1567361488613095\n",
            "Gradient Descent(664/999): loss=8.977712200285838, w0=-3.535773601379128, w1=1.1568670082881145\n",
            "Gradient Descent(665/999): loss=8.977541122394047, w0=-3.537071496605205, w1=1.156997395941608\n",
            "Gradient Descent(666/999): loss=8.977371275815862, w0=-3.5383647126702598, w1=1.1571273135226228\n",
            "Gradient Descent(667/999): loss=8.97720265168904, w0=-3.5396532664435645, w1=1.1572567627258599\n",
            "Gradient Descent(668/999): loss=8.977035241215125, w0=-3.5409371747335747, w1=1.157385745239912\n",
            "Gradient Descent(669/999): loss=8.976869035658993, w0=-3.5422164542881482, w1=1.1575142627472816\n",
            "Gradient Descent(670/999): loss=8.976704026348386, w0=-3.5434911217947627, w1=1.1576423169244083\n",
            "Gradient Descent(671/999): loss=8.976540204673457, w0=-3.5447611938807353, w1=1.1577699094416853\n",
            "Gradient Descent(672/999): loss=8.97637756208634, w0=-3.546026687113438, w1=1.157897041963486\n",
            "Gradient Descent(673/999): loss=8.976216090100682, w0=-3.5472876180005146, w1=1.1580237161481812\n",
            "Gradient Descent(674/999): loss=8.976055780291212, w0=-3.548544002990095, w1=1.1581499336481647\n",
            "Gradient Descent(675/999): loss=8.975896624293302, w0=-3.549795858471011, w1=1.1582756961098717\n",
            "Gradient Descent(676/999): loss=8.975738613802525, w0=-3.5510432007730097, w1=1.1584010051738032\n",
            "Gradient Descent(677/999): loss=8.975581740574222, w0=-3.552286046166966, w1=1.1585258624745443\n",
            "Gradient Descent(678/999): loss=8.975425996423075, w0=-3.5535244108650943, w1=1.1586502696407885\n",
            "Gradient Descent(679/999): loss=8.975271373222684, w0=-3.5547583110211627, w1=1.1587742282953564\n",
            "Gradient Descent(680/999): loss=8.975117862905131, w0=-3.5559877627307004, w1=1.1588977400552192\n",
            "Gradient Descent(681/999): loss=8.974965457460572, w0=-3.5572127820312103, w1=1.1590208065315173\n",
            "Gradient Descent(682/999): loss=8.974814148936806, w0=-3.5584333849023757, w1=1.1591434293295837\n",
            "Gradient Descent(683/999): loss=8.974663929438874, w0=-3.559649587266271, w1=1.1592656100489633\n",
            "Gradient Descent(684/999): loss=8.974514791128632, w0=-3.5608614049875684, w1=1.1593873502834344\n",
            "Gradient Descent(685/999): loss=8.974366726224359, w0=-3.5620688538737446, w1=1.1595086516210298\n",
            "Gradient Descent(686/999): loss=8.974219727000337, w0=-3.5632719496752876, w1=1.1596295156440564\n",
            "Gradient Descent(687/999): loss=8.974073785786457, w0=-3.5644707080859015, w1=1.1597499439291177\n",
            "Gradient Descent(688/999): loss=8.973928894967807, w0=-3.565665144742712, w1=1.1598699380471318\n",
            "Gradient Descent(689/999): loss=8.973785046984288, w0=-3.56685527522647, w1=1.1599894995633546\n",
            "Gradient Descent(690/999): loss=8.973642234330214, w0=-3.568041115061754, w1=1.1601086300373984\n",
            "Gradient Descent(691/999): loss=8.97350044955392, w0=-3.5692226797171744, w1=1.1602273310232527\n",
            "Gradient Descent(692/999): loss=8.973359685257375, w0=-3.570399984605574, w1=1.1603456040693048\n",
            "Gradient Descent(693/999): loss=8.97321993409579, w0=-3.571573045084229, w1=1.1604634507183598\n",
            "Gradient Descent(694/999): loss=8.97308118877724, w0=-3.57274187645505, w1=1.1605808725076605\n",
            "Gradient Descent(695/999): loss=8.972943442062286, w0=-3.5739064939647815, w1=1.1606978709689078\n",
            "Gradient Descent(696/999): loss=8.972806686763592, w0=-3.5750669128052, w1=1.1608144476282802\n",
            "Gradient Descent(697/999): loss=8.972670915745557, w0=-3.576223148113313, w1=1.1609306040064549\n",
            "Gradient Descent(698/999): loss=8.972536121923929, w0=-3.577375214971556, w1=1.1610463416186254\n",
            "Gradient Descent(699/999): loss=8.972402298265447, w0=-3.57852312840799, w1=1.161161661974524\n",
            "Gradient Descent(700/999): loss=8.972269437787478, w0=-3.5796669033964967, w1=1.161276566578439\n",
            "Gradient Descent(701/999): loss=8.972137533557634, w0=-3.580806554856974, w1=1.1613910569292365\n",
            "Gradient Descent(702/999): loss=8.972006578693431, w0=-3.5819420976555305, w1=1.1615051345203782\n",
            "Gradient Descent(703/999): loss=8.971876566361923, w0=-3.58307354660468, w1=1.1616188008399415\n",
            "Gradient Descent(704/999): loss=8.971747489779329, w0=-3.5842009164635336, w1=1.1617320573706398\n",
            "Gradient Descent(705/999): loss=8.97161934221071, w0=-3.585324221937994, w1=1.1618449055898397\n",
            "Gradient Descent(706/999): loss=8.971492116969593, w0=-3.586443477680946, w1=1.1619573469695834\n",
            "Gradient Descent(707/999): loss=8.971365807417625, w0=-3.5875586982924474, w1=1.162069382976604\n",
            "Gradient Descent(708/999): loss=8.971240406964244, w0=-3.5886698983199206, w1=1.1621810150723488\n",
            "Gradient Descent(709/999): loss=8.97111590906632, w0=-3.589777092258341, w1=1.1622922447129949\n",
            "Gradient Descent(710/999): loss=8.970992307227803, w0=-3.5908802945504283, w1=1.16240307334947\n",
            "Gradient Descent(711/999): loss=8.970869594999417, w0=-3.591979519586832, w1=1.162513502427471\n",
            "Gradient Descent(712/999): loss=8.970747765978288, w0=-3.593074781706321, w1=1.1626235333874835\n",
            "Gradient Descent(713/999): loss=8.970626813807637, w0=-3.5941660951959706, w1=1.1627331676647976\n",
            "Gradient Descent(714/999): loss=8.97050673217643, w0=-3.5952534742913476, w1=1.1628424066895322\n",
            "Gradient Descent(715/999): loss=8.97038751481906, w0=-3.5963369331766977, w1=1.1629512518866474\n",
            "Gradient Descent(716/999): loss=8.970269155515014, w0=-3.597416485985129, w1=1.1630597046759679\n",
            "Gradient Descent(717/999): loss=8.970151648088553, w0=-3.598492146798798, w1=1.1631677664721982\n",
            "Gradient Descent(718/999): loss=8.970034986408383, w0=-3.599563929649091, w1=1.1632754386849444\n",
            "Gradient Descent(719/999): loss=8.96991916438734, w0=-3.6006318485168096, w1=1.1633827227187286\n",
            "Gradient Descent(720/999): loss=8.96980417598208, w0=-3.6016959173323513, w1=1.1634896199730114\n",
            "Gradient Descent(721/999): loss=8.969690015192743, w0=-3.602756149975892, w1=1.163596131842206\n",
            "Gradient Descent(722/999): loss=8.969576676062658, w0=-3.603812560277567, w1=1.1637022597157\n",
            "Gradient Descent(723/999): loss=8.969464152678025, w0=-3.604865162017651, w1=1.1638080049778714\n",
            "Gradient Descent(724/999): loss=8.969352439167615, w0=-3.605913968926739, w1=1.1639133690081076\n",
            "Gradient Descent(725/999): loss=8.969241529702447, w0=-3.6069589946859235, w1=1.1640183531808224\n",
            "Gradient Descent(726/999): loss=8.969131418495499, w0=-3.6080002529269746, w1=1.1641229588654751\n",
            "Gradient Descent(727/999): loss=8.969022099801393, w0=-3.6090377572325174, w1=1.1642271874265875\n",
            "Gradient Descent(728/999): loss=8.968913567916113, w0=-3.6100715211362084, w1=1.164331040223763\n",
            "Gradient Descent(729/999): loss=8.968805817176694, w0=-3.611101558122914, w1=1.1644345186117016\n",
            "Gradient Descent(730/999): loss=8.968698841960922, w0=-3.612127881628883, w1=1.1645376239402212\n",
            "Gradient Descent(731/999): loss=8.968592636687061, w0=-3.6131505050419257, w1=1.1646403575542716\n",
            "Gradient Descent(732/999): loss=8.968487195813534, w0=-3.6141694417015864, w1=1.1647427207939558\n",
            "Gradient Descent(733/999): loss=8.968382513838664, w0=-3.6151847048993173, w1=1.1648447149945433\n",
            "Gradient Descent(734/999): loss=8.968278585300359, w0=-3.6161963078786528, w1=1.1649463414864913\n",
            "Gradient Descent(735/999): loss=8.968175404775852, w0=-3.617204263835381, w1=1.1650476015954603\n",
            "Gradient Descent(736/999): loss=8.968072966881392, w0=-3.6182085859177184, w1=1.1651484966423302\n",
            "Gradient Descent(737/999): loss=8.96797126627199, w0=-3.619209287226478, w1=1.1652490279432206\n",
            "Gradient Descent(738/999): loss=8.967870297641113, w0=-3.6202063808152425, w1=1.165349196809505\n",
            "Gradient Descent(739/999): loss=8.967770055720438, w0=-3.6211998796905336, w1=1.1654490045478303\n",
            "Gradient Descent(740/999): loss=8.967670535279547, w0=-3.6221897968119827, w1=1.165548452460131\n",
            "Gradient Descent(741/999): loss=8.96757173112567, w0=-3.623176145092499, w1=1.1656475418436496\n",
            "Gradient Descent(742/999): loss=8.967473638103419, w0=-3.624158937398437, w1=1.165746273990951\n",
            "Gradient Descent(743/999): loss=8.967376251094496, w0=-3.625138186549768, w1=1.1658446501899398\n",
            "Gradient Descent(744/999): loss=8.967279565017458, w0=-3.626113905320242, w1=1.1659426717238783\n",
            "Gradient Descent(745/999): loss=8.967183574827427, w0=-3.6270861064375595, w1=1.166040339871402\n",
            "Gradient Descent(746/999): loss=8.96708827551583, w0=-3.628054802583534, w1=1.1661376559065364\n",
            "Gradient Descent(747/999): loss=8.966993662110157, w0=-3.6290200063942586, w1=1.1662346210987142\n",
            "Gradient Descent(748/999): loss=8.96689972967367, w0=-3.6299817304602713, w1=1.1663312367127916\n",
            "Gradient Descent(749/999): loss=8.96680647330518, w0=-3.6309399873267187, w1=1.1664275040090644\n",
            "Gradient Descent(750/999): loss=8.966713888138756, w0=-3.63189478949352, w1=1.1665234242432856\n",
            "Gradient Descent(751/999): loss=8.966621969343501, w0=-3.6328461494155286, w1=1.1666189986666795\n",
            "Gradient Descent(752/999): loss=8.966530712123285, w0=-3.6337940795026977, w1=1.1667142285259615\n",
            "Gradient Descent(753/999): loss=8.9664401117165, w0=-3.6347385921202386, w1=1.1668091150633502\n",
            "Gradient Descent(754/999): loss=8.966350163395802, w0=-3.6356796995887843, w1=1.1669036595165878\n",
            "Gradient Descent(755/999): loss=8.966260862467877, w0=-3.63661741418455, w1=1.1669978631189522\n",
            "Gradient Descent(756/999): loss=8.966172204273187, w0=-3.6375517481394914, w1=1.1670917270992769\n",
            "Gradient Descent(757/999): loss=8.966084184185737, w0=-3.6384827136414675, w1=1.1671852526819642\n",
            "Gradient Descent(758/999): loss=8.965996797612824, w0=-3.639410322834396, w1=1.1672784410870025\n",
            "Gradient Descent(759/999): loss=8.965910039994794, w0=-3.6403345878184146, w1=1.1673712935299816\n",
            "Gradient Descent(760/999): loss=8.96582390680482, w0=-3.6412555206500374, w1=1.1674638112221094\n",
            "Gradient Descent(761/999): loss=8.965738393548648, w0=-3.6421731333423124, w1=1.1675559953702268\n",
            "Gradient Descent(762/999): loss=8.965653495764384, w0=-3.643087437864978, w1=1.1676478471768237\n",
            "Gradient Descent(763/999): loss=8.965569209022231, w0=-3.6439984461446198, w1=1.1677393678400552\n",
            "Gradient Descent(764/999): loss=8.965485528924285, w0=-3.644906170064825, w1=1.1678305585537565\n",
            "Gradient Descent(765/999): loss=8.965402451104294, w0=-3.6458106214663397, w1=1.1679214205074586\n",
            "Gradient Descent(766/999): loss=8.965319971227434, w0=-3.6467118121472204, w1=1.168011954886405\n",
            "Gradient Descent(767/999): loss=8.965238084990073, w0=-3.64760975386299, w1=1.1681021628715647\n",
            "Gradient Descent(768/999): loss=8.965156788119561, w0=-3.6485044583267903, w1=1.1681920456396506\n",
            "Gradient Descent(769/999): loss=8.965076076373997, w0=-3.649395937209535, w1=1.1682816043631317\n",
            "Gradient Descent(770/999): loss=8.96499594554201, w0=-3.650284202140062, w1=1.1683708402102517\n",
            "Gradient Descent(771/999): loss=8.964916391442541, w0=-3.651169264705285, w1=1.1684597543450415\n",
            "Gradient Descent(772/999): loss=8.964837409924625, w0=-3.652051136450345, w1=1.1685483479273353\n",
            "Gradient Descent(773/999): loss=8.96475899686717, w0=-3.6529298288787597, w1=1.1686366221127868\n",
            "Gradient Descent(774/999): loss=8.964681148178748, w0=-3.653805353452575, w1=1.1687245780528828\n",
            "Gradient Descent(775/999): loss=8.964603859797377, w0=-3.6546777215925137, w1=1.1688122168949586\n",
            "Gradient Descent(776/999): loss=8.96452712769031, w0=-3.6555469446781252, w1=1.168899539782213\n",
            "Gradient Descent(777/999): loss=8.964450947853827, w0=-3.656413034047933, w1=1.1689865478537247\n",
            "Gradient Descent(778/999): loss=8.964375316313022, w0=-3.657276000999583, w1=1.1690732422444636\n",
            "Gradient Descent(779/999): loss=8.964300229121605, w0=-3.658135856789991, w1=1.16915962408531\n",
            "Gradient Descent(780/999): loss=8.964225682361675, w0=-3.6589926126354895, w1=1.1692456945030654\n",
            "Gradient Descent(781/999): loss=8.964151672143544, w0=-3.659846279711974, w1=1.16933145462047\n",
            "Gradient Descent(782/999): loss=8.964078194605511, w0=-3.660696869155049, w1=1.1694169055562162\n",
            "Gradient Descent(783/999): loss=8.964005245913675, w0=-3.6615443920601725, w1=1.169502048424963\n",
            "Gradient Descent(784/999): loss=8.963932822261722, w0=-3.6623888594828014, w1=1.169586884337351\n",
            "Gradient Descent(785/999): loss=8.963860919870736, w0=-3.6632302824385357, w1=1.1696714144000164\n",
            "Gradient Descent(786/999): loss=8.963789534989006, w0=-3.6640686719032622, w1=1.1697556397156066\n",
            "Gradient Descent(787/999): loss=8.963718663891814, w0=-3.6649040388132974, w1=1.1698395613827928\n",
            "Gradient Descent(788/999): loss=8.96364830288126, w0=-3.66573639406553, w1=1.169923180496286\n",
            "Gradient Descent(789/999): loss=8.963578448286048, w0=-3.6665657485175633, w1=1.1700064981468499\n",
            "Gradient Descent(790/999): loss=8.963509096461312, w0=-3.6673921129878577, w1=1.1700895154213165\n",
            "Gradient Descent(791/999): loss=8.963440243788417, w0=-3.66821549825587, w1=1.1701722334025988\n",
            "Gradient Descent(792/999): loss=8.963371886674782, w0=-3.6690359150621954, w1=1.1702546531697062\n",
            "Gradient Descent(793/999): loss=8.963304021553673, w0=-3.669853374108707, w1=1.1703367757977585\n",
            "Gradient Descent(794/999): loss=8.963236644884025, w0=-3.670667886058696, w1=1.1704186023579979\n",
            "Gradient Descent(795/999): loss=8.963169753150266, w0=-3.67147946153701, w1=1.1705001339178067\n",
            "Gradient Descent(796/999): loss=8.963103342862132, w0=-3.6722881111301926, w1=1.170581371540717\n",
            "Gradient Descent(797/999): loss=8.963037410554467, w0=-3.6730938453866195, w1=1.1706623162864287\n",
            "Gradient Descent(798/999): loss=8.96297195278706, w0=-3.6738966748166395, w1=1.1707429692108189\n",
            "Gradient Descent(799/999): loss=8.962906966144464, w0=-3.674696609892708, w1=1.1708233313659606\n",
            "Gradient Descent(800/999): loss=8.962842447235813, w0=-3.6754936610495252, w1=1.1709034038001322\n",
            "Gradient Descent(801/999): loss=8.962778392694643, w0=-3.676287838684173, w1=1.1709831875578331\n",
            "Gradient Descent(802/999): loss=8.962714799178721, w0=-3.67707915315625, w1=1.1710626836797977\n",
            "Gradient Descent(803/999): loss=8.962651663369874, w0=-3.6778676147880054, w1=1.1711418932030078\n",
            "Gradient Descent(804/999): loss=8.96258898197381, w0=-3.6786532338644755, w1=1.1712208171607068\n",
            "Gradient Descent(805/999): loss=8.962526751719937, w0=-3.679436020633617, w1=1.171299456582413\n",
            "Gradient Descent(806/999): loss=8.962464969361212, w0=-3.6802159853064405, w1=1.1713778124939334\n",
            "Gradient Descent(807/999): loss=8.962403631673967, w0=-3.680993138057144, w1=1.1714558859173763\n",
            "Gradient Descent(808/999): loss=8.962342735457726, w0=-3.6817674890232452, w1=1.1715336778711656\n",
            "Gradient Descent(809/999): loss=8.96228227753505, w0=-3.682539048305715, w1=1.1716111893700532\n",
            "Gradient Descent(810/999): loss=8.962222254751373, w0=-3.6833078259691083, w1=1.1716884214251329\n",
            "Gradient Descent(811/999): loss=8.962162663974834, w0=-3.6840738320416944, w1=1.1717653750438528\n",
            "Gradient Descent(812/999): loss=8.962103502096106, w0=-3.6848370765155893, w1=1.17184205123003\n",
            "Gradient Descent(813/999): loss=8.962044766028251, w0=-3.6855975693468856, w1=1.1719184509838612\n",
            "Gradient Descent(814/999): loss=8.961986452706538, w0=-3.686355320455782, w1=1.171994575301939\n",
            "Gradient Descent(815/999): loss=8.961928559088307, w0=-3.687110339726714, w1=1.1720704251772607\n",
            "Gradient Descent(816/999): loss=8.961871082152784, w0=-3.68786263700848, w1=1.1721460015992464\n",
            "Gradient Descent(817/999): loss=8.961814018900945, w0=-3.688612222114373, w1=1.172221305553747\n",
            "Gradient Descent(818/999): loss=8.961757366355348, w0=-3.689359104822307, w1=1.1722963380230598\n",
            "Gradient Descent(819/999): loss=8.961701121559985, w0=-3.690103294874944, w1=1.1723710999859418\n",
            "Gradient Descent(820/999): loss=8.961645281580116, w0=-3.6908448019798232, w1=1.1724455924176198\n",
            "Gradient Descent(821/999): loss=8.961589843502132, w0=-3.6915836358094847, w1=1.1725198162898058\n",
            "Gradient Descent(822/999): loss=8.961534804433386, w0=-3.6923198060015983, w1=1.172593772570708\n",
            "Gradient Descent(823/999): loss=8.961480161502058, w0=-3.6930533221590878, w1=1.1726674622250446\n",
            "Gradient Descent(824/999): loss=8.961425911856988, w0=-3.6937841938502567, w1=1.172740886214055\n",
            "Gradient Descent(825/999): loss=8.96137205266755, w0=-3.6945124306089125, w1=1.1728140454955138\n",
            "Gradient Descent(826/999): loss=8.961318581123484, w0=-3.695238041934492, w1=1.1728869410237428\n",
            "Gradient Descent(827/999): loss=8.961265494434748, w0=-3.6959610372921854, w1=1.1729595737496223\n",
            "Gradient Descent(828/999): loss=8.9612127898314, w0=-3.696681426113057, w1=1.173031944620606\n",
            "Gradient Descent(829/999): loss=8.961160464563422, w0=-3.697399217794173, w1=1.1731040545807299\n",
            "Gradient Descent(830/999): loss=8.961108515900582, w0=-3.6981144216987185, w1=1.1731759045706285\n",
            "Gradient Descent(831/999): loss=8.961056941132322, w0=-3.6988270471561244, w1=1.1732474955275438\n",
            "Gradient Descent(832/999): loss=8.961005737567566, w0=-3.699537103462187, w1=1.1733188283853397\n",
            "Gradient Descent(833/999): loss=8.960954902534624, w0=-3.7002445998791895, w1=1.1733899040745128\n",
            "Gradient Descent(834/999): loss=8.960904433381032, w0=-3.700949545636022, w1=1.1734607235222052\n",
            "Gradient Descent(835/999): loss=8.960854327473411, w0=-3.7016519499283036, w1=1.1735312876522168\n",
            "Gradient Descent(836/999): loss=8.960804582197348, w0=-3.702351821918501, w1=1.1736015973850167\n",
            "Gradient Descent(837/999): loss=8.960755194957226, w0=-3.7030491707360484, w1=1.1736716536377554\n",
            "Gradient Descent(838/999): loss=8.960706163176129, w0=-3.703744005477467, w1=1.173741457324277\n",
            "Gradient Descent(839/999): loss=8.960657484295684, w0=-3.7044363352064824, w1=1.1738110093551317\n",
            "Gradient Descent(840/999): loss=8.960609155775924, w0=-3.705126168954145, w1=1.1738803106375857\n",
            "Gradient Descent(841/999): loss=8.960561175095169, w0=-3.7058135157189454, w1=1.1739493620756356\n",
            "Gradient Descent(842/999): loss=8.96051353974989, w0=-3.706498384466934, w1=1.174018164570018\n",
            "Gradient Descent(843/999): loss=8.960466247254569, w0=-3.7071807841318365, w1=1.1740867190182227\n",
            "Gradient Descent(844/999): loss=8.96041929514159, w0=-3.7078607236151697, w1=1.1741550263145037\n",
            "Gradient Descent(845/999): loss=8.960372680961084, w0=-3.7085382117863603, w1=1.1742230873498911\n",
            "Gradient Descent(846/999): loss=8.960326402280824, w0=-3.7092132574828582, w1=1.1742909030122024\n",
            "Gradient Descent(847/999): loss=8.960280456686082, w0=-3.7098858695102526, w1=1.1743584741860547\n",
            "Gradient Descent(848/999): loss=8.960234841779515, w0=-3.710556056642387, w1=1.174425801752876\n",
            "Gradient Descent(849/999): loss=8.960189555181033, w0=-3.711223827621474, w1=1.1744928865909157\n",
            "Gradient Descent(850/999): loss=8.960144594527675, w0=-3.7118891911582077, w1=1.1745597295752581\n",
            "Gradient Descent(851/999): loss=8.96009995747349, w0=-3.7125521559318795, w1=1.174626331577832\n",
            "Gradient Descent(852/999): loss=8.960055641689406, w0=-3.71321273059049, w1=1.1746926934674229\n",
            "Gradient Descent(853/999): loss=8.960011644863123, w0=-3.7138709237508616, w1=1.174758816109684\n",
            "Gradient Descent(854/999): loss=8.959967964698974, w0=-3.7145267439987526, w1=1.1748247003671477\n",
            "Gradient Descent(855/999): loss=8.959924598917821, w0=-3.715180199888967, w1=1.1748903470992373\n",
            "Gradient Descent(856/999): loss=8.959881545256932, w0=-3.715831299945467, w1=1.1749557571622766\n",
            "Gradient Descent(857/999): loss=8.959838801469852, w0=-3.7164800526614843, w1=1.175020931409504\n",
            "Gradient Descent(858/999): loss=8.959796365326303, w0=-3.717126466499632, w1=1.1750858706910792\n",
            "Gradient Descent(859/999): loss=8.959754234612053, w0=-3.717770549892013, w1=1.1751505758540997\n",
            "Gradient Descent(860/999): loss=8.959712407128809, w0=-3.7184123112403307, w1=1.1752150477426069\n",
            "Gradient Descent(861/999): loss=8.959670880694105, w0=-3.719051758915999, w1=1.1752792871976006\n",
            "Gradient Descent(862/999): loss=8.959629653141173, w0=-3.7196889012602505, w1=1.1753432950570482\n",
            "Gradient Descent(863/999): loss=8.959588722318852, w0=-3.720323746584248, w1=1.1754070721558956\n",
            "Gradient Descent(864/999): loss=8.959548086091454, w0=-3.7209563031691886, w1=1.1754706193260793\n",
            "Gradient Descent(865/999): loss=8.95950774233866, w0=-3.721586579266416, w1=1.1755339373965363\n",
            "Gradient Descent(866/999): loss=8.959467688955428, w0=-3.722214583097525, w1=1.1755970271932146\n",
            "Gradient Descent(867/999): loss=8.959427923851846, w0=-3.7228403228544704, w1=1.1756598895390855\n",
            "Gradient Descent(868/999): loss=8.959388444953062, w0=-3.723463806699674, w1=1.175722525254152\n",
            "Gradient Descent(869/999): loss=8.959349250199146, w0=-3.7240850427661294, w1=1.1757849351554626\n",
            "Gradient Descent(870/999): loss=8.959310337544999, w0=-3.7247040391575097, w1=1.1758471200571183\n",
            "Gradient Descent(871/999): loss=8.959271704960237, w0=-3.7253208039482733, w1=1.175909080770286\n",
            "Gradient Descent(872/999): loss=8.959233350429098, w0=-3.7259353451837676, w1=1.175970818103209\n",
            "Gradient Descent(873/999): loss=8.95919527195032, w0=-3.726547670880336, w1=1.1760323328612146\n",
            "Gradient Descent(874/999): loss=8.959157467537045, w0=-3.72715778902542, w1=1.1760936258467294\n",
            "Gradient Descent(875/999): loss=8.95911993521672, w0=-3.727765707577667, w1=1.1761546978592845\n",
            "Gradient Descent(876/999): loss=8.959082673030991, w0=-3.7283714344670296, w1=1.1762155496955309\n",
            "Gradient Descent(877/999): loss=8.959045679035587, w0=-3.728974977594873, w1=1.1762761821492451\n",
            "Gradient Descent(878/999): loss=8.959008951300246, w0=-3.729576344834076, w1=1.1763365960113448\n",
            "Gradient Descent(879/999): loss=8.958972487908586, w0=-3.730175544029134, w1=1.176396792069893\n",
            "Gradient Descent(880/999): loss=8.958936286958028, w0=-3.730772582996262, w1=1.1764567711101144\n",
            "Gradient Descent(881/999): loss=8.958900346559684, w0=-3.731367469523495, w1=1.176516533914401\n",
            "Gradient Descent(882/999): loss=8.958864664838252, w0=-3.731960211370792, w1=1.1765760812623252\n",
            "Gradient Descent(883/999): loss=8.958829239931935, w0=-3.732550816270135, w1=1.1766354139306485\n",
            "Gradient Descent(884/999): loss=8.958794069992342, w0=-3.7331392919256303, w1=1.1766945326933316\n",
            "Gradient Descent(885/999): loss=8.958759153184374, w0=-3.733725646013611, w1=1.1767534383215457\n",
            "Gradient Descent(886/999): loss=8.95872448768615, w0=-3.7343098861827335, w1=1.176812131583681\n",
            "Gradient Descent(887/999): loss=8.958690071688888, w0=-3.7348920200540814, w1=1.1768706132453584\n",
            "Gradient Descent(888/999): loss=8.958655903396842, w0=-3.7354720552212615, w1=1.1769288840694379\n",
            "Gradient Descent(889/999): loss=8.958621981027179, w0=-3.7360499992505045, w1=1.1769869448160293\n",
            "Gradient Descent(890/999): loss=8.958588302809899, w0=-3.736625859680763, w1=1.1770447962425021\n",
            "Gradient Descent(891/999): loss=8.958554866987745, w0=-3.7371996440238116, w1=1.177102439103496\n",
            "Gradient Descent(892/999): loss=8.958521671816104, w0=-3.737771359764342, w1=1.1771598741509293\n",
            "Gradient Descent(893/999): loss=8.958488715562924, w0=-3.738341014360062, w1=1.1772171021340092\n",
            "Gradient Descent(894/999): loss=8.958455996508615, w0=-3.7389086152417947, w1=1.1772741237992432\n",
            "Gradient Descent(895/999): loss=8.958423512945966, w0=-3.7394741698135725, w1=1.177330939890446\n",
            "Gradient Descent(896/999): loss=8.95839126318005, w0=-3.7400376854527346, w1=1.1773875511487517\n",
            "Gradient Descent(897/999): loss=8.958359245528147, w0=-3.740599169510024, w1=1.177443958312622\n",
            "Gradient Descent(898/999): loss=8.958327458319637, w0=-3.741158629309682, w1=1.1775001621178565\n",
            "Gradient Descent(899/999): loss=8.958295899895933, w0=-3.741716072149546, w1=1.1775561632976024\n",
            "Gradient Descent(900/999): loss=8.958264568610385, w0=-3.742271505301143, w1=1.1776119625823624\n",
            "Gradient Descent(901/999): loss=8.958233462828186, w0=-3.742824936009783, w1=1.1776675607000082\n",
            "Gradient Descent(902/999): loss=8.958202580926304, w0=-3.743376371494658, w1=1.1777229583757842\n",
            "Gradient Descent(903/999): loss=8.958171921293388, w0=-3.7439258189489317, w1=1.177778156332323\n",
            "Gradient Descent(904/999): loss=8.958141482329681, w0=-3.744473285539835, w1=1.1778331552896497\n",
            "Gradient Descent(905/999): loss=8.958111262446941, w0=-3.74501877840876, w1=1.1778879559651958\n",
            "Gradient Descent(906/999): loss=8.958081260068361, w0=-3.745562304671353, w1=1.1779425590738035\n",
            "Gradient Descent(907/999): loss=8.958051473628474, w0=-3.7461038714176067, w1=1.1779969653277411\n",
            "Gradient Descent(908/999): loss=8.95802190157309, w0=-3.7466434857119526, w1=1.1780511754367065\n",
            "Gradient Descent(909/999): loss=8.9579925423592, w0=-3.7471811545933544, w1=1.17810519010784\n",
            "Gradient Descent(910/999): loss=8.957963394454898, w0=-3.747716885075399, w1=1.1781590100457326\n",
            "Gradient Descent(911/999): loss=8.957934456339315, w0=-3.7482506841463863, w1=1.178212635952435\n",
            "Gradient Descent(912/999): loss=8.957905726502515, w0=-3.7487825587694243, w1=1.1782660685274666\n",
            "Gradient Descent(913/999): loss=8.957877203445435, w0=-3.7493125158825165, w1=1.1783193084678254\n",
            "Gradient Descent(914/999): loss=8.957848885679805, w0=-3.7498405623986537, w1=1.178372356467996\n",
            "Gradient Descent(915/999): loss=8.957820771728063, w0=-3.750366705205904, w1=1.17842521321996\n",
            "Gradient Descent(916/999): loss=8.957792860123279, w0=-3.7508909511675026, w1=1.1784778794132031\n",
            "Gradient Descent(917/999): loss=8.957765149409084, w0=-3.751413307121942, w1=1.1785303557347269\n",
            "Gradient Descent(918/999): loss=8.957737638139594, w0=-3.7519337798830596, w1=1.178582642869055\n",
            "Gradient Descent(919/999): loss=8.957710324879326, w0=-3.7524523762401287, w1=1.178634741498243\n",
            "Gradient Descent(920/999): loss=8.95768320820313, w0=-3.7529691029579455, w1=1.1786866523018886\n",
            "Gradient Descent(921/999): loss=8.957656286696116, w0=-3.7534839667769178, w1=1.1787383759571386\n",
            "Gradient Descent(922/999): loss=8.95762955895358, w0=-3.7539969744131527, w1=1.1787899131386992\n",
            "Gradient Descent(923/999): loss=8.957603023580916, w0=-3.754508132558545, w1=1.1788412645188435\n",
            "Gradient Descent(924/999): loss=8.957576679193572, w0=-3.7550174478808636, w1=1.1788924307674211\n",
            "Gradient Descent(925/999): loss=8.957550524416948, w0=-3.7555249270238384, w1=1.1789434125518672\n",
            "Gradient Descent(926/999): loss=8.957524557886344, w0=-3.756030576607248, w1=1.17899421053721\n",
            "Gradient Descent(927/999): loss=8.957498778246881, w0=-3.756534403227006, w1=1.1790448253860806\n",
            "Gradient Descent(928/999): loss=8.95747318415343, w0=-3.7570364134552445, w1=1.1790952577587208\n",
            "Gradient Descent(929/999): loss=8.95744777427055, w0=-3.757536613840404, w1=1.179145508312993\n",
            "Gradient Descent(930/999): loss=8.957422547272397, w0=-3.758035010907315, w1=1.1791955777043868\n",
            "Gradient Descent(931/999): loss=8.957397501842687, w0=-3.758531611157286, w1=1.179245466586029\n",
            "Gradient Descent(932/999): loss=8.957372636674599, w0=-3.759026421068186, w1=1.1792951756086922\n",
            "Gradient Descent(933/999): loss=8.957347950470718, w0=-3.759519447094531, w1=1.1793447054208024\n",
            "Gradient Descent(934/999): loss=8.957323441942975, w0=-3.7600106956675656, w1=1.1793940566684478\n",
            "Gradient Descent(935/999): loss=8.957299109812563, w0=-3.7605001731953505, w1=1.1794432299953879\n",
            "Gradient Descent(936/999): loss=8.957274952809883, w0=-3.7609878860628427, w1=1.1794922260430603\n",
            "Gradient Descent(937/999): loss=8.957250969674472, w0=-3.7614738406319814, w1=1.1795410454505912\n",
            "Gradient Descent(938/999): loss=8.957227159154941, w0=-3.7619580432417687, w1=1.1795896888548023\n",
            "Gradient Descent(939/999): loss=8.957203520008909, w0=-3.7624405002083536, w1=1.1796381568902188\n",
            "Gradient Descent(940/999): loss=8.957180051002933, w0=-3.762921217825115, w1=1.1796864501890791\n",
            "Gradient Descent(941/999): loss=8.957156750912446, w0=-3.7634002023627415, w1=1.1797345693813421\n",
            "Gradient Descent(942/999): loss=8.957133618521702, w0=-3.7638774600693163, w1=1.1797825150946952\n",
            "Gradient Descent(943/999): loss=8.957110652623701, w0=-3.764352997170396, w1=1.1798302879545635\n",
            "Gradient Descent(944/999): loss=8.957087852020134, w0=-3.7648268198690933, w1=1.1798778885841164\n",
            "Gradient Descent(945/999): loss=8.957065215521304, w0=-3.765298934346157, w1=1.1799253176042774\n",
            "Gradient Descent(946/999): loss=8.957042741946093, w0=-3.7657693467600537, w1=1.1799725756337314\n",
            "Gradient Descent(947/999): loss=8.957020430121872, w0=-3.766238063247047, w1=1.1800196632889326\n",
            "Gradient Descent(948/999): loss=8.956998278884463, w0=-3.766705089921279, w1=1.1800665811841127\n",
            "Gradient Descent(949/999): loss=8.956976287078056, w0=-3.767170432874848, w1=1.1801133299312891\n",
            "Gradient Descent(950/999): loss=8.956954453555166, w0=-3.76763409817789, w1=1.1801599101402733\n",
            "Gradient Descent(951/999): loss=8.956932777176567, w0=-3.7680960918786566, w1=1.180206322418677\n",
            "Gradient Descent(952/999): loss=8.956911256811228, w0=-3.768556420003594, w1=1.1802525673719226\n",
            "Gradient Descent(953/999): loss=8.956889891336262, w0=-3.7690150885574223, w1=1.1802986456032492\n",
            "Gradient Descent(954/999): loss=8.956868679636862, w0=-3.769472103523214, w1=1.1803445577137215\n",
            "Gradient Descent(955/999): loss=8.956847620606252, w0=-3.7699274708624704, w1=1.1803903043022363\n",
            "Gradient Descent(956/999): loss=8.95682671314561, w0=-3.770381196515201, w1=1.1804358859655333\n",
            "Gradient Descent(957/999): loss=8.956805956164027, w0=-3.7708332864000003, w1=1.180481303298198\n",
            "Gradient Descent(958/999): loss=8.956785348578453, w0=-3.771283746414125, w1=1.180526556892675\n",
            "Gradient Descent(959/999): loss=8.956764889313623, w0=-3.7717325824335717, w1=1.1805716473392711\n",
            "Gradient Descent(960/999): loss=8.95674457730202, w0=-3.772179800313152, w1=1.1806165752261664\n",
            "Gradient Descent(961/999): loss=8.9567244114838, w0=-3.7726254058865707, w1=1.180661341139419\n",
            "Gradient Descent(962/999): loss=8.956704390806754, w0=-3.7730694049665, w1=1.1807059456629752\n",
            "Gradient Descent(963/999): loss=8.956684514226248, w0=-3.773511803344657, w1=1.180750389378676\n",
            "Gradient Descent(964/999): loss=8.956664780705161, w0=-3.7739526067918785, w1=1.1807946728662644\n",
            "Gradient Descent(965/999): loss=8.956645189213841, w0=-3.774391821058196, w1=1.1808387967033933\n",
            "Gradient Descent(966/999): loss=8.956625738730041, w0=-3.7748294518729115, w1=1.1808827614656336\n",
            "Gradient Descent(967/999): loss=8.956606428238882, w0=-3.775265504944671, w1=1.1809265677264802\n",
            "Gradient Descent(968/999): loss=8.956587256732778, w0=-3.7756999859615403, w1=1.1809702160573616\n",
            "Gradient Descent(969/999): loss=8.956568223211404, w0=-3.776132900591079, w1=1.1810137070276452\n",
            "Gradient Descent(970/999): loss=8.956549326681625, w0=-3.776564254480413, w1=1.1810570412046462\n",
            "Gradient Descent(971/999): loss=8.956530566157465, w0=-3.7769940532563107, w1=1.1811002191536342\n",
            "Gradient Descent(972/999): loss=8.956511940660032, w0=-3.777422302525253, w1=1.1811432414378418\n",
            "Gradient Descent(973/999): loss=8.956493449217492, w0=-3.77784900787351, w1=1.1811861086184694\n",
            "Gradient Descent(974/999): loss=8.956475090865004, w0=-3.778274174867212, w1=1.1812288212546962\n",
            "Gradient Descent(975/999): loss=8.95645686464466, w0=-3.7786978090524213, w1=1.1812713799036836\n",
            "Gradient Descent(976/999): loss=8.956438769605457, w0=-3.7791199159552065, w1=1.181313785120585\n",
            "Gradient Descent(977/999): loss=8.956420804803233, w0=-3.7795405010817134, w1=1.1813560374585537\n",
            "Gradient Descent(978/999): loss=8.956402969300624, w0=-3.7799595699182373, w1=1.1813981374687457\n",
            "Gradient Descent(979/999): loss=8.956385262167009, w0=-3.780377127931294, w1=1.1814400857003333\n",
            "Gradient Descent(980/999): loss=8.956367682478469, w0=-3.780793180567692, w1=1.181481882700507\n",
            "Gradient Descent(981/999): loss=8.95635022931773, w0=-3.781207733254602, w1=1.1815235290144848\n",
            "Gradient Descent(982/999): loss=8.956332901774122, w0=-3.78162079139963, w1=1.1815650251855196\n",
            "Gradient Descent(983/999): loss=8.956315698943532, w0=-3.7820323603908856, w1=1.1816063717549052\n",
            "Gradient Descent(984/999): loss=8.956298619928353, w0=-3.7824424455970536, w1=1.181647569261985\n",
            "Gradient Descent(985/999): loss=8.956281663837439, w0=-3.7828510523674637, w1=1.1816886182441557\n",
            "Gradient Descent(986/999): loss=8.956264829786052, w0=-3.78325818603216, w1=1.1817295192368797\n",
            "Gradient Descent(987/999): loss=8.956248116895827, w0=-3.7836638519019705, w1=1.181770272773686\n",
            "Gradient Descent(988/999): loss=8.956231524294726, w0=-3.7840680552685777, w1=1.1818108793861823\n",
            "Gradient Descent(989/999): loss=8.956215051116974, w0=-3.7844708014045856, w1=1.1818513396040586\n",
            "Gradient Descent(990/999): loss=8.95619869650304, w0=-3.78487209556359, w1=1.1818916539550954\n",
            "Gradient Descent(991/999): loss=8.956182459599576, w0=-3.785271942980246, w1=1.181931822965171\n",
            "Gradient Descent(992/999): loss=8.956166339559369, w0=-3.7856703488703376, w1=1.181971847158267\n",
            "Gradient Descent(993/999): loss=8.956150335541311, w0=-3.7860673184308435, w1=1.1820117270564765\n",
            "Gradient Descent(994/999): loss=8.95613444671035, w0=-3.7864628568400076, w1=1.1820514631800105\n",
            "Gradient Descent(995/999): loss=8.956118672237434, w0=-3.786856969257405, w1=1.1820910560472038\n",
            "Gradient Descent(996/999): loss=8.956103011299486, w0=-3.7872496608240085, w1=1.1821305061745233\n",
            "Gradient Descent(997/999): loss=8.956087463079355, w0=-3.787640936662258, w1=1.1821698140765735\n",
            "Gradient Descent(998/999): loss=8.95607202676576, w0=-3.7880308018761255, w1=1.1822089802661038\n",
            "Gradient Descent(999/999): loss=8.956056701553273, w0=-3.7884192615511822, w1=1.1822480052540147\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-3.78841926,  1.18224801])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we can compute the cost (error) of the trained model using our fitted parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.956041486642253"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "computeCost(X, y, g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's plot the linear model along with the data to visually see how well it fits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
        "f = g[0] + (g[1] * x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.plot(x, f, 'r', label='Prediction')\n",
        "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
        "ax.legend(loc=2)\n",
        "ax.set_xlabel('Population')\n",
        "ax.set_ylabel('Profit')\n",
        "ax.set_title('Predicted Profit vs. Population Size')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Error vs. Training Epoch')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.plot(np.arange(iters), cost, 'r')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_ylabel('Cost')\n",
        "ax.set_title('Error vs. Training Epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
